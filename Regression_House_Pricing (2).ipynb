{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression House Pricing",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK9IW4QCa2wq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "69801118-b2ff-4090-f463-cfd45ea0360a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras import models\n",
        "from keras import layers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkc87yf6cF9L",
        "colab_type": "code",
        "outputId": "b4b22520-b0bb-42ad-bbe1-84c5b9607992",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import io\n",
        "from google.colab import files\n",
        "uploaded= files.upload()\n",
        "df= pd.read_csv(io.StringIO(uploaded['housing.csv'].decode('utf-8')) , delim_whitespace = True , header = None)\n",
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-998eddad-ef91-4d75-a3cc-ce3f0c929e2d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-998eddad-ef91-4d75-a3cc-ce3f0c929e2d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving housing.csv to housing.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.02985</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.430</td>\n",
              "      <td>58.7</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.12</td>\n",
              "      <td>5.21</td>\n",
              "      <td>28.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "      <td>22.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.14455</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.172</td>\n",
              "      <td>96.1</td>\n",
              "      <td>5.9505</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>19.15</td>\n",
              "      <td>27.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.21124</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.631</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.0821</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.63</td>\n",
              "      <td>29.93</td>\n",
              "      <td>16.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.17004</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.004</td>\n",
              "      <td>85.9</td>\n",
              "      <td>6.5921</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.71</td>\n",
              "      <td>17.10</td>\n",
              "      <td>18.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.22489</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.377</td>\n",
              "      <td>94.3</td>\n",
              "      <td>6.3467</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>392.52</td>\n",
              "      <td>20.45</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.11747</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.009</td>\n",
              "      <td>82.9</td>\n",
              "      <td>6.2267</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>13.27</td>\n",
              "      <td>18.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.09378</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.889</td>\n",
              "      <td>39.0</td>\n",
              "      <td>5.4509</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>390.50</td>\n",
              "      <td>15.71</td>\n",
              "      <td>21.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.62976</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.949</td>\n",
              "      <td>61.8</td>\n",
              "      <td>4.7075</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>8.26</td>\n",
              "      <td>20.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.63796</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.096</td>\n",
              "      <td>84.5</td>\n",
              "      <td>4.4619</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>380.02</td>\n",
              "      <td>10.26</td>\n",
              "      <td>18.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.62739</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.834</td>\n",
              "      <td>56.5</td>\n",
              "      <td>4.4986</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>395.62</td>\n",
              "      <td>8.47</td>\n",
              "      <td>19.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.05393</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.935</td>\n",
              "      <td>29.3</td>\n",
              "      <td>4.4986</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>386.85</td>\n",
              "      <td>6.58</td>\n",
              "      <td>23.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.78420</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.990</td>\n",
              "      <td>81.7</td>\n",
              "      <td>4.2579</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>386.75</td>\n",
              "      <td>14.67</td>\n",
              "      <td>17.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.80271</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.456</td>\n",
              "      <td>36.6</td>\n",
              "      <td>3.7965</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>288.99</td>\n",
              "      <td>11.69</td>\n",
              "      <td>20.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.72580</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.727</td>\n",
              "      <td>69.5</td>\n",
              "      <td>3.7965</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>390.95</td>\n",
              "      <td>11.28</td>\n",
              "      <td>18.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.25179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.570</td>\n",
              "      <td>98.1</td>\n",
              "      <td>3.7979</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>376.57</td>\n",
              "      <td>21.02</td>\n",
              "      <td>13.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.85204</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.965</td>\n",
              "      <td>89.2</td>\n",
              "      <td>4.0123</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>392.53</td>\n",
              "      <td>13.83</td>\n",
              "      <td>19.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.23247</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.142</td>\n",
              "      <td>91.7</td>\n",
              "      <td>3.9769</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>18.72</td>\n",
              "      <td>15.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.98843</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.813</td>\n",
              "      <td>100.0</td>\n",
              "      <td>4.0952</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>394.54</td>\n",
              "      <td>19.88</td>\n",
              "      <td>14.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.75026</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.924</td>\n",
              "      <td>94.1</td>\n",
              "      <td>4.3996</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>394.33</td>\n",
              "      <td>16.30</td>\n",
              "      <td>15.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.84054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.599</td>\n",
              "      <td>85.7</td>\n",
              "      <td>4.4546</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>303.42</td>\n",
              "      <td>16.51</td>\n",
              "      <td>13.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.67191</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.813</td>\n",
              "      <td>90.3</td>\n",
              "      <td>4.6820</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>376.88</td>\n",
              "      <td>14.81</td>\n",
              "      <td>16.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.95577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.047</td>\n",
              "      <td>88.8</td>\n",
              "      <td>4.4534</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>306.38</td>\n",
              "      <td>17.28</td>\n",
              "      <td>14.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.77299</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.495</td>\n",
              "      <td>94.4</td>\n",
              "      <td>4.4547</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>387.94</td>\n",
              "      <td>12.80</td>\n",
              "      <td>18.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1.00245</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.674</td>\n",
              "      <td>87.3</td>\n",
              "      <td>4.2390</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>380.23</td>\n",
              "      <td>11.98</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>4.87141</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.614</td>\n",
              "      <td>6.484</td>\n",
              "      <td>93.6</td>\n",
              "      <td>2.3053</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>396.21</td>\n",
              "      <td>18.68</td>\n",
              "      <td>16.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>15.02340</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.614</td>\n",
              "      <td>5.304</td>\n",
              "      <td>97.3</td>\n",
              "      <td>2.1007</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>349.48</td>\n",
              "      <td>24.91</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>10.23300</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.614</td>\n",
              "      <td>6.185</td>\n",
              "      <td>96.7</td>\n",
              "      <td>2.1705</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>379.70</td>\n",
              "      <td>18.03</td>\n",
              "      <td>14.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>14.33370</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.614</td>\n",
              "      <td>6.229</td>\n",
              "      <td>88.0</td>\n",
              "      <td>1.9512</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>383.32</td>\n",
              "      <td>13.11</td>\n",
              "      <td>21.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>5.82401</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.532</td>\n",
              "      <td>6.242</td>\n",
              "      <td>64.7</td>\n",
              "      <td>3.4242</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>10.74</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>5.70818</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.532</td>\n",
              "      <td>6.750</td>\n",
              "      <td>74.9</td>\n",
              "      <td>3.3317</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>393.07</td>\n",
              "      <td>7.74</td>\n",
              "      <td>23.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>5.73116</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.532</td>\n",
              "      <td>7.061</td>\n",
              "      <td>77.0</td>\n",
              "      <td>3.4106</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>395.28</td>\n",
              "      <td>7.01</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>2.81838</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.532</td>\n",
              "      <td>5.762</td>\n",
              "      <td>40.3</td>\n",
              "      <td>4.0983</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>392.92</td>\n",
              "      <td>10.42</td>\n",
              "      <td>21.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>484</th>\n",
              "      <td>2.37857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.583</td>\n",
              "      <td>5.871</td>\n",
              "      <td>41.9</td>\n",
              "      <td>3.7240</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>370.73</td>\n",
              "      <td>13.34</td>\n",
              "      <td>20.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>485</th>\n",
              "      <td>3.67367</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.583</td>\n",
              "      <td>6.312</td>\n",
              "      <td>51.9</td>\n",
              "      <td>3.9917</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>388.62</td>\n",
              "      <td>10.58</td>\n",
              "      <td>21.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>5.69175</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.583</td>\n",
              "      <td>6.114</td>\n",
              "      <td>79.8</td>\n",
              "      <td>3.5459</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>392.68</td>\n",
              "      <td>14.98</td>\n",
              "      <td>19.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>4.83567</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.583</td>\n",
              "      <td>5.905</td>\n",
              "      <td>53.2</td>\n",
              "      <td>3.1523</td>\n",
              "      <td>24</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>388.22</td>\n",
              "      <td>11.45</td>\n",
              "      <td>20.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>0.15086</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0.609</td>\n",
              "      <td>5.454</td>\n",
              "      <td>92.7</td>\n",
              "      <td>1.8209</td>\n",
              "      <td>4</td>\n",
              "      <td>711.0</td>\n",
              "      <td>20.1</td>\n",
              "      <td>395.09</td>\n",
              "      <td>18.06</td>\n",
              "      <td>15.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>0.18337</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0.609</td>\n",
              "      <td>5.414</td>\n",
              "      <td>98.3</td>\n",
              "      <td>1.7554</td>\n",
              "      <td>4</td>\n",
              "      <td>711.0</td>\n",
              "      <td>20.1</td>\n",
              "      <td>344.05</td>\n",
              "      <td>23.97</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>0.20746</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0.609</td>\n",
              "      <td>5.093</td>\n",
              "      <td>98.0</td>\n",
              "      <td>1.8226</td>\n",
              "      <td>4</td>\n",
              "      <td>711.0</td>\n",
              "      <td>20.1</td>\n",
              "      <td>318.43</td>\n",
              "      <td>29.68</td>\n",
              "      <td>8.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>0.10574</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0.609</td>\n",
              "      <td>5.983</td>\n",
              "      <td>98.8</td>\n",
              "      <td>1.8681</td>\n",
              "      <td>4</td>\n",
              "      <td>711.0</td>\n",
              "      <td>20.1</td>\n",
              "      <td>390.11</td>\n",
              "      <td>18.07</td>\n",
              "      <td>13.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>0.11132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0.609</td>\n",
              "      <td>5.983</td>\n",
              "      <td>83.5</td>\n",
              "      <td>2.1099</td>\n",
              "      <td>4</td>\n",
              "      <td>711.0</td>\n",
              "      <td>20.1</td>\n",
              "      <td>396.90</td>\n",
              "      <td>13.35</td>\n",
              "      <td>20.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>0.17331</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0</td>\n",
              "      <td>0.585</td>\n",
              "      <td>5.707</td>\n",
              "      <td>54.0</td>\n",
              "      <td>2.3817</td>\n",
              "      <td>6</td>\n",
              "      <td>391.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>12.01</td>\n",
              "      <td>21.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>0.27957</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0</td>\n",
              "      <td>0.585</td>\n",
              "      <td>5.926</td>\n",
              "      <td>42.6</td>\n",
              "      <td>2.3817</td>\n",
              "      <td>6</td>\n",
              "      <td>391.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>13.59</td>\n",
              "      <td>24.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>0.17899</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0</td>\n",
              "      <td>0.585</td>\n",
              "      <td>5.670</td>\n",
              "      <td>28.8</td>\n",
              "      <td>2.7986</td>\n",
              "      <td>6</td>\n",
              "      <td>391.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>393.29</td>\n",
              "      <td>17.60</td>\n",
              "      <td>23.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>0.28960</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0</td>\n",
              "      <td>0.585</td>\n",
              "      <td>5.390</td>\n",
              "      <td>72.9</td>\n",
              "      <td>2.7986</td>\n",
              "      <td>6</td>\n",
              "      <td>391.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>21.14</td>\n",
              "      <td>19.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>0.26838</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0</td>\n",
              "      <td>0.585</td>\n",
              "      <td>5.794</td>\n",
              "      <td>70.6</td>\n",
              "      <td>2.8927</td>\n",
              "      <td>6</td>\n",
              "      <td>391.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>14.10</td>\n",
              "      <td>18.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>0.23912</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0</td>\n",
              "      <td>0.585</td>\n",
              "      <td>6.019</td>\n",
              "      <td>65.3</td>\n",
              "      <td>2.4091</td>\n",
              "      <td>6</td>\n",
              "      <td>391.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>12.92</td>\n",
              "      <td>21.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>0.17783</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0</td>\n",
              "      <td>0.585</td>\n",
              "      <td>5.569</td>\n",
              "      <td>73.5</td>\n",
              "      <td>2.3999</td>\n",
              "      <td>6</td>\n",
              "      <td>391.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>395.77</td>\n",
              "      <td>15.10</td>\n",
              "      <td>17.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>0.22438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0</td>\n",
              "      <td>0.585</td>\n",
              "      <td>6.027</td>\n",
              "      <td>79.7</td>\n",
              "      <td>2.4982</td>\n",
              "      <td>6</td>\n",
              "      <td>391.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>14.33</td>\n",
              "      <td>16.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>0.06263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.593</td>\n",
              "      <td>69.1</td>\n",
              "      <td>2.4786</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>391.99</td>\n",
              "      <td>9.67</td>\n",
              "      <td>22.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>0.04527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.120</td>\n",
              "      <td>76.7</td>\n",
              "      <td>2.2875</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.08</td>\n",
              "      <td>20.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>0.06076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.976</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2.1675</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.64</td>\n",
              "      <td>23.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>0.10959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.794</td>\n",
              "      <td>89.3</td>\n",
              "      <td>2.3889</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>393.45</td>\n",
              "      <td>6.48</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.030</td>\n",
              "      <td>80.8</td>\n",
              "      <td>2.5050</td>\n",
              "      <td>1</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>7.88</td>\n",
              "      <td>11.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows  14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0     1      2   3      4   ...     9     10      11     12    13\n",
              "0     0.00632  18.0   2.31   0  0.538  ...  296.0  15.3  396.90   4.98  24.0\n",
              "1     0.02731   0.0   7.07   0  0.469  ...  242.0  17.8  396.90   9.14  21.6\n",
              "2     0.02729   0.0   7.07   0  0.469  ...  242.0  17.8  392.83   4.03  34.7\n",
              "3     0.03237   0.0   2.18   0  0.458  ...  222.0  18.7  394.63   2.94  33.4\n",
              "4     0.06905   0.0   2.18   0  0.458  ...  222.0  18.7  396.90   5.33  36.2\n",
              "5     0.02985   0.0   2.18   0  0.458  ...  222.0  18.7  394.12   5.21  28.7\n",
              "6     0.08829  12.5   7.87   0  0.524  ...  311.0  15.2  395.60  12.43  22.9\n",
              "7     0.14455  12.5   7.87   0  0.524  ...  311.0  15.2  396.90  19.15  27.1\n",
              "8     0.21124  12.5   7.87   0  0.524  ...  311.0  15.2  386.63  29.93  16.5\n",
              "9     0.17004  12.5   7.87   0  0.524  ...  311.0  15.2  386.71  17.10  18.9\n",
              "10    0.22489  12.5   7.87   0  0.524  ...  311.0  15.2  392.52  20.45  15.0\n",
              "11    0.11747  12.5   7.87   0  0.524  ...  311.0  15.2  396.90  13.27  18.9\n",
              "12    0.09378  12.5   7.87   0  0.524  ...  311.0  15.2  390.50  15.71  21.7\n",
              "13    0.62976   0.0   8.14   0  0.538  ...  307.0  21.0  396.90   8.26  20.4\n",
              "14    0.63796   0.0   8.14   0  0.538  ...  307.0  21.0  380.02  10.26  18.2\n",
              "15    0.62739   0.0   8.14   0  0.538  ...  307.0  21.0  395.62   8.47  19.9\n",
              "16    1.05393   0.0   8.14   0  0.538  ...  307.0  21.0  386.85   6.58  23.1\n",
              "17    0.78420   0.0   8.14   0  0.538  ...  307.0  21.0  386.75  14.67  17.5\n",
              "18    0.80271   0.0   8.14   0  0.538  ...  307.0  21.0  288.99  11.69  20.2\n",
              "19    0.72580   0.0   8.14   0  0.538  ...  307.0  21.0  390.95  11.28  18.2\n",
              "20    1.25179   0.0   8.14   0  0.538  ...  307.0  21.0  376.57  21.02  13.6\n",
              "21    0.85204   0.0   8.14   0  0.538  ...  307.0  21.0  392.53  13.83  19.6\n",
              "22    1.23247   0.0   8.14   0  0.538  ...  307.0  21.0  396.90  18.72  15.2\n",
              "23    0.98843   0.0   8.14   0  0.538  ...  307.0  21.0  394.54  19.88  14.5\n",
              "24    0.75026   0.0   8.14   0  0.538  ...  307.0  21.0  394.33  16.30  15.6\n",
              "25    0.84054   0.0   8.14   0  0.538  ...  307.0  21.0  303.42  16.51  13.9\n",
              "26    0.67191   0.0   8.14   0  0.538  ...  307.0  21.0  376.88  14.81  16.6\n",
              "27    0.95577   0.0   8.14   0  0.538  ...  307.0  21.0  306.38  17.28  14.8\n",
              "28    0.77299   0.0   8.14   0  0.538  ...  307.0  21.0  387.94  12.80  18.4\n",
              "29    1.00245   0.0   8.14   0  0.538  ...  307.0  21.0  380.23  11.98  21.0\n",
              "..        ...   ...    ...  ..    ...  ...    ...   ...     ...    ...   ...\n",
              "476   4.87141   0.0  18.10   0  0.614  ...  666.0  20.2  396.21  18.68  16.7\n",
              "477  15.02340   0.0  18.10   0  0.614  ...  666.0  20.2  349.48  24.91  12.0\n",
              "478  10.23300   0.0  18.10   0  0.614  ...  666.0  20.2  379.70  18.03  14.6\n",
              "479  14.33370   0.0  18.10   0  0.614  ...  666.0  20.2  383.32  13.11  21.4\n",
              "480   5.82401   0.0  18.10   0  0.532  ...  666.0  20.2  396.90  10.74  23.0\n",
              "481   5.70818   0.0  18.10   0  0.532  ...  666.0  20.2  393.07   7.74  23.7\n",
              "482   5.73116   0.0  18.10   0  0.532  ...  666.0  20.2  395.28   7.01  25.0\n",
              "483   2.81838   0.0  18.10   0  0.532  ...  666.0  20.2  392.92  10.42  21.8\n",
              "484   2.37857   0.0  18.10   0  0.583  ...  666.0  20.2  370.73  13.34  20.6\n",
              "485   3.67367   0.0  18.10   0  0.583  ...  666.0  20.2  388.62  10.58  21.2\n",
              "486   5.69175   0.0  18.10   0  0.583  ...  666.0  20.2  392.68  14.98  19.1\n",
              "487   4.83567   0.0  18.10   0  0.583  ...  666.0  20.2  388.22  11.45  20.6\n",
              "488   0.15086   0.0  27.74   0  0.609  ...  711.0  20.1  395.09  18.06  15.2\n",
              "489   0.18337   0.0  27.74   0  0.609  ...  711.0  20.1  344.05  23.97   7.0\n",
              "490   0.20746   0.0  27.74   0  0.609  ...  711.0  20.1  318.43  29.68   8.1\n",
              "491   0.10574   0.0  27.74   0  0.609  ...  711.0  20.1  390.11  18.07  13.6\n",
              "492   0.11132   0.0  27.74   0  0.609  ...  711.0  20.1  396.90  13.35  20.1\n",
              "493   0.17331   0.0   9.69   0  0.585  ...  391.0  19.2  396.90  12.01  21.8\n",
              "494   0.27957   0.0   9.69   0  0.585  ...  391.0  19.2  396.90  13.59  24.5\n",
              "495   0.17899   0.0   9.69   0  0.585  ...  391.0  19.2  393.29  17.60  23.1\n",
              "496   0.28960   0.0   9.69   0  0.585  ...  391.0  19.2  396.90  21.14  19.7\n",
              "497   0.26838   0.0   9.69   0  0.585  ...  391.0  19.2  396.90  14.10  18.3\n",
              "498   0.23912   0.0   9.69   0  0.585  ...  391.0  19.2  396.90  12.92  21.2\n",
              "499   0.17783   0.0   9.69   0  0.585  ...  391.0  19.2  395.77  15.10  17.5\n",
              "500   0.22438   0.0   9.69   0  0.585  ...  391.0  19.2  396.90  14.33  16.8\n",
              "501   0.06263   0.0  11.93   0  0.573  ...  273.0  21.0  391.99   9.67  22.4\n",
              "502   0.04527   0.0  11.93   0  0.573  ...  273.0  21.0  396.90   9.08  20.6\n",
              "503   0.06076   0.0  11.93   0  0.573  ...  273.0  21.0  396.90   5.64  23.9\n",
              "504   0.10959   0.0  11.93   0  0.573  ...  273.0  21.0  393.45   6.48  22.0\n",
              "505   0.04741   0.0  11.93   0  0.573  ...  273.0  21.0  396.90   7.88  11.9\n",
              "\n",
              "[506 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVjSYZQidSVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset= df.values\n",
        "x = dataset[:,0:13]\n",
        "y= dataset[:,13]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfM0ikT5Ig_D",
        "colab_type": "code",
        "outputId": "79df8ed5-e9a3-45ec-c988-2ec1e47cacaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s=7\n",
        "np.random.seed(s)\n",
        "s"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2R8AAW1dLXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create baseline Model\n",
        "def baseline_model():\n",
        "  model= Sequential()\n",
        "  model.add(Dense(13 , activation='relu' , input_shape=(13,)))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(optimizer='Adam' , loss= 'mse' , metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf8d4LJgNABO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = KerasRegressor(build_fn = baseline_model , epochs = 100 , batch_size = 5 , verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_YZ1_XENEHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = KFold(n_splits = 10 , random_state = 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqucfruRNJlX",
        "colab_type": "code",
        "outputId": "70aa4ddd-2bed-4346-8f00-61e0729d35e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "results = cross_val_score(estimator , x , y , cv = kfold)\n",
        "print(\"Results: %.2f (%.2f) MSE\" % (abs(results.mean()), results.std()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "455/455 [==============================] - 0s 735us/step - loss: 123990.7803 - mean_absolute_error: 338.5712\n",
            "Epoch 2/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 22409.0930 - mean_absolute_error: 137.2516\n",
            "Epoch 3/100\n",
            "455/455 [==============================] - 0s 218us/step - loss: 2457.9712 - mean_absolute_error: 40.0756\n",
            "Epoch 4/100\n",
            "455/455 [==============================] - 0s 181us/step - loss: 798.5908 - mean_absolute_error: 21.6243\n",
            "Epoch 5/100\n",
            "455/455 [==============================] - 0s 213us/step - loss: 563.5787 - mean_absolute_error: 17.7248\n",
            "Epoch 6/100\n",
            "455/455 [==============================] - 0s 213us/step - loss: 408.2978 - mean_absolute_error: 15.0526\n",
            "Epoch 7/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 304.4820 - mean_absolute_error: 13.1324\n",
            "Epoch 8/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 238.9395 - mean_absolute_error: 11.4249\n",
            "Epoch 9/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 200.4117 - mean_absolute_error: 10.5988\n",
            "Epoch 10/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 175.5958 - mean_absolute_error: 9.9033\n",
            "Epoch 11/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 159.2185 - mean_absolute_error: 9.5101\n",
            "Epoch 12/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 146.5860 - mean_absolute_error: 9.2097\n",
            "Epoch 13/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 136.1810 - mean_absolute_error: 8.8232\n",
            "Epoch 14/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 127.2486 - mean_absolute_error: 8.5194\n",
            "Epoch 15/100\n",
            "455/455 [==============================] - 0s 240us/step - loss: 119.1562 - mean_absolute_error: 8.1869\n",
            "Epoch 16/100\n",
            "455/455 [==============================] - 0s 256us/step - loss: 111.4940 - mean_absolute_error: 7.9522\n",
            "Epoch 17/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 104.4741 - mean_absolute_error: 7.6923\n",
            "Epoch 18/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 97.6384 - mean_absolute_error: 7.4016\n",
            "Epoch 19/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 93.1548 - mean_absolute_error: 7.2593\n",
            "Epoch 20/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 87.4975 - mean_absolute_error: 7.0364\n",
            "Epoch 21/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 82.0068 - mean_absolute_error: 6.7596\n",
            "Epoch 22/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 76.1078 - mean_absolute_error: 6.5421\n",
            "Epoch 23/100\n",
            "455/455 [==============================] - 0s 209us/step - loss: 71.9318 - mean_absolute_error: 6.3650\n",
            "Epoch 24/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 68.2593 - mean_absolute_error: 6.1430\n",
            "Epoch 25/100\n",
            "455/455 [==============================] - 0s 186us/step - loss: 65.3602 - mean_absolute_error: 6.0121\n",
            "Epoch 26/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 62.0654 - mean_absolute_error: 5.8383\n",
            "Epoch 27/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 58.6176 - mean_absolute_error: 5.6586\n",
            "Epoch 28/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 57.2582 - mean_absolute_error: 5.5455\n",
            "Epoch 29/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 55.1246 - mean_absolute_error: 5.4427\n",
            "Epoch 30/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 53.8921 - mean_absolute_error: 5.3852\n",
            "Epoch 31/100\n",
            "455/455 [==============================] - 0s 183us/step - loss: 52.4635 - mean_absolute_error: 5.3427\n",
            "Epoch 32/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 51.4996 - mean_absolute_error: 5.2725\n",
            "Epoch 33/100\n",
            "455/455 [==============================] - 0s 186us/step - loss: 51.1127 - mean_absolute_error: 5.1960\n",
            "Epoch 34/100\n",
            "455/455 [==============================] - 0s 248us/step - loss: 50.5824 - mean_absolute_error: 5.1867\n",
            "Epoch 35/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 49.0830 - mean_absolute_error: 5.0903\n",
            "Epoch 36/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 49.4412 - mean_absolute_error: 5.2231\n",
            "Epoch 37/100\n",
            "455/455 [==============================] - 0s 214us/step - loss: 47.6329 - mean_absolute_error: 5.1135\n",
            "Epoch 38/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 48.1716 - mean_absolute_error: 5.0705\n",
            "Epoch 39/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 48.4042 - mean_absolute_error: 5.2081\n",
            "Epoch 40/100\n",
            "455/455 [==============================] - 0s 184us/step - loss: 47.4417 - mean_absolute_error: 5.0837\n",
            "Epoch 41/100\n",
            "455/455 [==============================] - 0s 213us/step - loss: 46.1103 - mean_absolute_error: 5.0427\n",
            "Epoch 42/100\n",
            "455/455 [==============================] - 0s 182us/step - loss: 47.6104 - mean_absolute_error: 5.1150\n",
            "Epoch 43/100\n",
            "455/455 [==============================] - 0s 185us/step - loss: 45.6950 - mean_absolute_error: 4.9474\n",
            "Epoch 44/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 45.4333 - mean_absolute_error: 4.9937\n",
            "Epoch 45/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 45.9303 - mean_absolute_error: 5.0548\n",
            "Epoch 46/100\n",
            "455/455 [==============================] - 0s 186us/step - loss: 44.7223 - mean_absolute_error: 4.9838\n",
            "Epoch 47/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 45.7493 - mean_absolute_error: 5.0264\n",
            "Epoch 48/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 44.6511 - mean_absolute_error: 4.9583\n",
            "Epoch 49/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 44.4409 - mean_absolute_error: 4.9231\n",
            "Epoch 50/100\n",
            "455/455 [==============================] - 0s 177us/step - loss: 43.8154 - mean_absolute_error: 4.8430\n",
            "Epoch 51/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 43.6629 - mean_absolute_error: 4.8760\n",
            "Epoch 52/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 44.1914 - mean_absolute_error: 4.9376\n",
            "Epoch 53/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 44.1370 - mean_absolute_error: 4.9117\n",
            "Epoch 54/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 43.5979 - mean_absolute_error: 4.8351\n",
            "Epoch 55/100\n",
            "455/455 [==============================] - 0s 230us/step - loss: 43.3763 - mean_absolute_error: 4.8990\n",
            "Epoch 56/100\n",
            "455/455 [==============================] - 0s 232us/step - loss: 45.3302 - mean_absolute_error: 5.0404\n",
            "Epoch 57/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 41.9483 - mean_absolute_error: 4.7490\n",
            "Epoch 58/100\n",
            "455/455 [==============================] - 0s 241us/step - loss: 42.3290 - mean_absolute_error: 4.8613\n",
            "Epoch 59/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 40.7068 - mean_absolute_error: 4.7462\n",
            "Epoch 60/100\n",
            "455/455 [==============================] - 0s 183us/step - loss: 40.1318 - mean_absolute_error: 4.6630\n",
            "Epoch 61/100\n",
            "455/455 [==============================] - 0s 220us/step - loss: 39.9934 - mean_absolute_error: 4.6666\n",
            "Epoch 62/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 39.6670 - mean_absolute_error: 4.6801\n",
            "Epoch 63/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 39.5069 - mean_absolute_error: 4.6707\n",
            "Epoch 64/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 40.0472 - mean_absolute_error: 4.6191\n",
            "Epoch 65/100\n",
            "455/455 [==============================] - 0s 241us/step - loss: 39.2834 - mean_absolute_error: 4.6732\n",
            "Epoch 66/100\n",
            "455/455 [==============================] - 0s 186us/step - loss: 40.3240 - mean_absolute_error: 4.6574\n",
            "Epoch 67/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 38.0365 - mean_absolute_error: 4.6020\n",
            "Epoch 68/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 39.0696 - mean_absolute_error: 4.6024\n",
            "Epoch 69/100\n",
            "455/455 [==============================] - 0s 215us/step - loss: 39.5720 - mean_absolute_error: 4.6760\n",
            "Epoch 70/100\n",
            "455/455 [==============================] - 0s 219us/step - loss: 40.4909 - mean_absolute_error: 4.7713\n",
            "Epoch 71/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 37.7160 - mean_absolute_error: 4.5191\n",
            "Epoch 72/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 37.0374 - mean_absolute_error: 4.4823\n",
            "Epoch 73/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 39.3658 - mean_absolute_error: 4.6240\n",
            "Epoch 74/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 40.3231 - mean_absolute_error: 4.7035\n",
            "Epoch 75/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 38.5658 - mean_absolute_error: 4.6188\n",
            "Epoch 76/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 36.6410 - mean_absolute_error: 4.3951\n",
            "Epoch 77/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 36.7228 - mean_absolute_error: 4.4706\n",
            "Epoch 78/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 36.4093 - mean_absolute_error: 4.4191\n",
            "Epoch 79/100\n",
            "455/455 [==============================] - 0s 180us/step - loss: 38.3425 - mean_absolute_error: 4.4896\n",
            "Epoch 80/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 35.5971 - mean_absolute_error: 4.5003\n",
            "Epoch 81/100\n",
            "455/455 [==============================] - 0s 230us/step - loss: 38.7835 - mean_absolute_error: 4.5878\n",
            "Epoch 82/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 36.2424 - mean_absolute_error: 4.3915\n",
            "Epoch 83/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 36.4129 - mean_absolute_error: 4.3738\n",
            "Epoch 84/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 34.5541 - mean_absolute_error: 4.3560\n",
            "Epoch 85/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 36.0534 - mean_absolute_error: 4.3579\n",
            "Epoch 86/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 34.5108 - mean_absolute_error: 4.2555\n",
            "Epoch 87/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 34.0251 - mean_absolute_error: 4.2701\n",
            "Epoch 88/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 36.4089 - mean_absolute_error: 4.3649\n",
            "Epoch 89/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 35.8881 - mean_absolute_error: 4.4168\n",
            "Epoch 90/100\n",
            "455/455 [==============================] - 0s 218us/step - loss: 35.7840 - mean_absolute_error: 4.3610\n",
            "Epoch 91/100\n",
            "455/455 [==============================] - 0s 230us/step - loss: 35.6199 - mean_absolute_error: 4.4178\n",
            "Epoch 92/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 34.7848 - mean_absolute_error: 4.3495\n",
            "Epoch 93/100\n",
            "455/455 [==============================] - 0s 181us/step - loss: 33.6567 - mean_absolute_error: 4.3661\n",
            "Epoch 94/100\n",
            "455/455 [==============================] - 0s 183us/step - loss: 35.7471 - mean_absolute_error: 4.3919\n",
            "Epoch 95/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 38.1863 - mean_absolute_error: 4.6508\n",
            "Epoch 96/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 34.8868 - mean_absolute_error: 4.3918\n",
            "Epoch 97/100\n",
            "455/455 [==============================] - 0s 182us/step - loss: 34.5553 - mean_absolute_error: 4.2993\n",
            "Epoch 98/100\n",
            "455/455 [==============================] - 0s 187us/step - loss: 34.8878 - mean_absolute_error: 4.3897\n",
            "Epoch 99/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 35.0084 - mean_absolute_error: 4.4481\n",
            "Epoch 100/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 33.7844 - mean_absolute_error: 4.2557\n",
            "51/51 [==============================] - 0s 1ms/step\n",
            "Epoch 1/100\n",
            "455/455 [==============================] - 0s 740us/step - loss: 1028.4982 - mean_absolute_error: 23.7233\n",
            "Epoch 2/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 180.3763 - mean_absolute_error: 10.9632\n",
            "Epoch 3/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 122.2095 - mean_absolute_error: 8.6083\n",
            "Epoch 4/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 98.5351 - mean_absolute_error: 7.4441\n",
            "Epoch 5/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 86.7116 - mean_absolute_error: 6.9393\n",
            "Epoch 6/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 80.3669 - mean_absolute_error: 6.5953\n",
            "Epoch 7/100\n",
            "455/455 [==============================] - 0s 225us/step - loss: 74.5588 - mean_absolute_error: 6.3494\n",
            "Epoch 8/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 71.2136 - mean_absolute_error: 6.2115\n",
            "Epoch 9/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 66.9632 - mean_absolute_error: 5.9751\n",
            "Epoch 10/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 64.5851 - mean_absolute_error: 5.8449\n",
            "Epoch 11/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 61.2754 - mean_absolute_error: 5.6819\n",
            "Epoch 12/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 60.7726 - mean_absolute_error: 5.7052\n",
            "Epoch 13/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 57.3824 - mean_absolute_error: 5.4223\n",
            "Epoch 14/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 54.6637 - mean_absolute_error: 5.3284\n",
            "Epoch 15/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 54.0272 - mean_absolute_error: 5.3587\n",
            "Epoch 16/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 54.1528 - mean_absolute_error: 5.2909\n",
            "Epoch 17/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 51.6741 - mean_absolute_error: 5.2681\n",
            "Epoch 18/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 51.5584 - mean_absolute_error: 5.1917\n",
            "Epoch 19/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 49.6440 - mean_absolute_error: 5.0859\n",
            "Epoch 20/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 49.4254 - mean_absolute_error: 5.0813\n",
            "Epoch 21/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 47.7159 - mean_absolute_error: 5.0278\n",
            "Epoch 22/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 46.2054 - mean_absolute_error: 4.9419\n",
            "Epoch 23/100\n",
            "455/455 [==============================] - 0s 187us/step - loss: 48.0297 - mean_absolute_error: 5.0620\n",
            "Epoch 24/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 44.9476 - mean_absolute_error: 4.9414\n",
            "Epoch 25/100\n",
            "455/455 [==============================] - 0s 214us/step - loss: 44.9106 - mean_absolute_error: 4.8610\n",
            "Epoch 26/100\n",
            "455/455 [==============================] - 0s 187us/step - loss: 44.8983 - mean_absolute_error: 4.9025\n",
            "Epoch 27/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 43.7665 - mean_absolute_error: 4.8633\n",
            "Epoch 28/100\n",
            "455/455 [==============================] - 0s 242us/step - loss: 45.4692 - mean_absolute_error: 4.9795\n",
            "Epoch 29/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 45.0901 - mean_absolute_error: 4.9524\n",
            "Epoch 30/100\n",
            "455/455 [==============================] - 0s 184us/step - loss: 43.7464 - mean_absolute_error: 4.8851\n",
            "Epoch 31/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 42.2705 - mean_absolute_error: 4.7736\n",
            "Epoch 32/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 42.7971 - mean_absolute_error: 4.8332\n",
            "Epoch 33/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 40.3525 - mean_absolute_error: 4.6827\n",
            "Epoch 34/100\n",
            "455/455 [==============================] - 0s 218us/step - loss: 39.2768 - mean_absolute_error: 4.5915\n",
            "Epoch 35/100\n",
            "455/455 [==============================] - 0s 223us/step - loss: 39.8327 - mean_absolute_error: 4.6737\n",
            "Epoch 36/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 41.9329 - mean_absolute_error: 4.8219\n",
            "Epoch 37/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 39.0588 - mean_absolute_error: 4.6002\n",
            "Epoch 38/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 40.1811 - mean_absolute_error: 4.6777\n",
            "Epoch 39/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 38.8748 - mean_absolute_error: 4.5830\n",
            "Epoch 40/100\n",
            "455/455 [==============================] - 0s 187us/step - loss: 37.8781 - mean_absolute_error: 4.4904\n",
            "Epoch 41/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 38.2125 - mean_absolute_error: 4.5035\n",
            "Epoch 42/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 37.5524 - mean_absolute_error: 4.5366\n",
            "Epoch 43/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 39.1949 - mean_absolute_error: 4.6710\n",
            "Epoch 44/100\n",
            "455/455 [==============================] - 0s 186us/step - loss: 37.6575 - mean_absolute_error: 4.4939\n",
            "Epoch 45/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 36.2923 - mean_absolute_error: 4.4144\n",
            "Epoch 46/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 38.4079 - mean_absolute_error: 4.6266\n",
            "Epoch 47/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 37.7813 - mean_absolute_error: 4.5624\n",
            "Epoch 48/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 38.5876 - mean_absolute_error: 4.5878\n",
            "Epoch 49/100\n",
            "455/455 [==============================] - 0s 219us/step - loss: 38.2220 - mean_absolute_error: 4.6940\n",
            "Epoch 50/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 36.6700 - mean_absolute_error: 4.4992\n",
            "Epoch 51/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 37.4820 - mean_absolute_error: 4.4917\n",
            "Epoch 52/100\n",
            "455/455 [==============================] - 0s 227us/step - loss: 38.9972 - mean_absolute_error: 4.5507\n",
            "Epoch 53/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 42.8678 - mean_absolute_error: 4.8943\n",
            "Epoch 54/100\n",
            "455/455 [==============================] - 0s 218us/step - loss: 36.8768 - mean_absolute_error: 4.4109\n",
            "Epoch 55/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 37.9084 - mean_absolute_error: 4.5157\n",
            "Epoch 56/100\n",
            "455/455 [==============================] - 0s 235us/step - loss: 35.6027 - mean_absolute_error: 4.3720\n",
            "Epoch 57/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 35.8680 - mean_absolute_error: 4.3775\n",
            "Epoch 58/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 35.2795 - mean_absolute_error: 4.3743\n",
            "Epoch 59/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 38.9945 - mean_absolute_error: 4.6740\n",
            "Epoch 60/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 35.6837 - mean_absolute_error: 4.4113\n",
            "Epoch 61/100\n",
            "455/455 [==============================] - 0s 182us/step - loss: 36.8430 - mean_absolute_error: 4.4718\n",
            "Epoch 62/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 36.2686 - mean_absolute_error: 4.4571\n",
            "Epoch 63/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 36.4193 - mean_absolute_error: 4.4402\n",
            "Epoch 64/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 37.9532 - mean_absolute_error: 4.5493\n",
            "Epoch 65/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 36.1302 - mean_absolute_error: 4.4172\n",
            "Epoch 66/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 34.8680 - mean_absolute_error: 4.3638\n",
            "Epoch 67/100\n",
            "455/455 [==============================] - 0s 227us/step - loss: 34.9173 - mean_absolute_error: 4.3199\n",
            "Epoch 68/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 34.4562 - mean_absolute_error: 4.2956\n",
            "Epoch 69/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 38.2819 - mean_absolute_error: 4.6029\n",
            "Epoch 70/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 34.3785 - mean_absolute_error: 4.3095\n",
            "Epoch 71/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 35.4427 - mean_absolute_error: 4.3070\n",
            "Epoch 72/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 34.0734 - mean_absolute_error: 4.2672\n",
            "Epoch 73/100\n",
            "455/455 [==============================] - 0s 225us/step - loss: 34.6647 - mean_absolute_error: 4.2761\n",
            "Epoch 74/100\n",
            "455/455 [==============================] - 0s 229us/step - loss: 35.2166 - mean_absolute_error: 4.3495\n",
            "Epoch 75/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 35.2926 - mean_absolute_error: 4.3949\n",
            "Epoch 76/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 34.4576 - mean_absolute_error: 4.2865\n",
            "Epoch 77/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 34.3088 - mean_absolute_error: 4.3265\n",
            "Epoch 78/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 34.7511 - mean_absolute_error: 4.4913\n",
            "Epoch 79/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 33.6678 - mean_absolute_error: 4.2411\n",
            "Epoch 80/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 33.4337 - mean_absolute_error: 4.1671\n",
            "Epoch 81/100\n",
            "455/455 [==============================] - 0s 184us/step - loss: 35.9230 - mean_absolute_error: 4.4069\n",
            "Epoch 82/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 34.4782 - mean_absolute_error: 4.2898\n",
            "Epoch 83/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 36.6754 - mean_absolute_error: 4.5984\n",
            "Epoch 84/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 34.7444 - mean_absolute_error: 4.3243\n",
            "Epoch 85/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 34.6608 - mean_absolute_error: 4.3744\n",
            "Epoch 86/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 32.3295 - mean_absolute_error: 4.1335\n",
            "Epoch 87/100\n",
            "455/455 [==============================] - 0s 187us/step - loss: 35.3599 - mean_absolute_error: 4.4372\n",
            "Epoch 88/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 33.5894 - mean_absolute_error: 4.2191\n",
            "Epoch 89/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 33.6844 - mean_absolute_error: 4.2615\n",
            "Epoch 90/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 32.5313 - mean_absolute_error: 4.1545\n",
            "Epoch 91/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 34.1361 - mean_absolute_error: 4.3245\n",
            "Epoch 92/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 33.9801 - mean_absolute_error: 4.2528\n",
            "Epoch 93/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 32.2615 - mean_absolute_error: 4.1816\n",
            "Epoch 94/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 32.4323 - mean_absolute_error: 4.1777\n",
            "Epoch 95/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 34.0737 - mean_absolute_error: 4.2819\n",
            "Epoch 96/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 34.9951 - mean_absolute_error: 4.4407\n",
            "Epoch 97/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 34.6273 - mean_absolute_error: 4.4364\n",
            "Epoch 98/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 32.5634 - mean_absolute_error: 4.1826\n",
            "Epoch 99/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 32.0410 - mean_absolute_error: 4.2274\n",
            "Epoch 100/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 32.5236 - mean_absolute_error: 4.1817\n",
            "51/51 [==============================] - 0s 2ms/step\n",
            "Epoch 1/100\n",
            "455/455 [==============================] - 0s 1ms/step - loss: 6244.4549 - mean_absolute_error: 69.4839\n",
            "Epoch 2/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 2060.4566 - mean_absolute_error: 37.5966\n",
            "Epoch 3/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 537.5020 - mean_absolute_error: 17.9714\n",
            "Epoch 4/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 155.6444 - mean_absolute_error: 9.4332\n",
            "Epoch 5/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 118.2375 - mean_absolute_error: 8.1113\n",
            "Epoch 6/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 109.3268 - mean_absolute_error: 7.7645\n",
            "Epoch 7/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 102.8286 - mean_absolute_error: 7.5661\n",
            "Epoch 8/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 97.1669 - mean_absolute_error: 7.3247\n",
            "Epoch 9/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 91.9820 - mean_absolute_error: 7.1290\n",
            "Epoch 10/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 86.8844 - mean_absolute_error: 6.9019\n",
            "Epoch 11/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 81.3108 - mean_absolute_error: 6.5417\n",
            "Epoch 12/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 70.3343 - mean_absolute_error: 6.1113\n",
            "Epoch 13/100\n",
            "455/455 [==============================] - 0s 214us/step - loss: 64.8141 - mean_absolute_error: 5.9281\n",
            "Epoch 14/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 65.3232 - mean_absolute_error: 5.8238\n",
            "Epoch 15/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 62.0083 - mean_absolute_error: 5.7909\n",
            "Epoch 16/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 60.7592 - mean_absolute_error: 5.7074\n",
            "Epoch 17/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 61.2437 - mean_absolute_error: 5.8274\n",
            "Epoch 18/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 59.5144 - mean_absolute_error: 5.6802\n",
            "Epoch 19/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 58.3667 - mean_absolute_error: 5.6130\n",
            "Epoch 20/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 55.3840 - mean_absolute_error: 5.5548\n",
            "Epoch 21/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 56.3384 - mean_absolute_error: 5.5524\n",
            "Epoch 22/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 59.8604 - mean_absolute_error: 5.7699\n",
            "Epoch 23/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 55.1087 - mean_absolute_error: 5.4573\n",
            "Epoch 24/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 54.6751 - mean_absolute_error: 5.5300\n",
            "Epoch 25/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 51.9260 - mean_absolute_error: 5.3815\n",
            "Epoch 26/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 51.9384 - mean_absolute_error: 5.3469\n",
            "Epoch 27/100\n",
            "455/455 [==============================] - 0s 228us/step - loss: 50.2323 - mean_absolute_error: 5.3111\n",
            "Epoch 28/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 51.0007 - mean_absolute_error: 5.3641\n",
            "Epoch 29/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 50.6610 - mean_absolute_error: 5.3397\n",
            "Epoch 30/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 50.3392 - mean_absolute_error: 5.4029\n",
            "Epoch 31/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 55.2741 - mean_absolute_error: 5.5929\n",
            "Epoch 32/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 49.8250 - mean_absolute_error: 5.2708\n",
            "Epoch 33/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 49.3437 - mean_absolute_error: 5.2348\n",
            "Epoch 34/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 47.8896 - mean_absolute_error: 5.2516\n",
            "Epoch 35/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 50.1266 - mean_absolute_error: 5.3557\n",
            "Epoch 36/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 49.6111 - mean_absolute_error: 5.3056\n",
            "Epoch 37/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 48.2626 - mean_absolute_error: 5.2252\n",
            "Epoch 38/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 48.7827 - mean_absolute_error: 5.2643\n",
            "Epoch 39/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 46.6584 - mean_absolute_error: 5.1846\n",
            "Epoch 40/100\n",
            "455/455 [==============================] - 0s 185us/step - loss: 48.3060 - mean_absolute_error: 5.2036\n",
            "Epoch 41/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 46.5995 - mean_absolute_error: 5.2339\n",
            "Epoch 42/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 45.6288 - mean_absolute_error: 5.0652\n",
            "Epoch 43/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 45.3433 - mean_absolute_error: 4.9816\n",
            "Epoch 44/100\n",
            "455/455 [==============================] - 0s 185us/step - loss: 44.4649 - mean_absolute_error: 5.0288\n",
            "Epoch 45/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 46.3085 - mean_absolute_error: 5.2155\n",
            "Epoch 46/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 48.0192 - mean_absolute_error: 5.1523\n",
            "Epoch 47/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 44.2091 - mean_absolute_error: 5.1064\n",
            "Epoch 48/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 42.9411 - mean_absolute_error: 4.8610\n",
            "Epoch 49/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 42.5820 - mean_absolute_error: 4.9390\n",
            "Epoch 50/100\n",
            "455/455 [==============================] - 0s 184us/step - loss: 42.9828 - mean_absolute_error: 4.9255\n",
            "Epoch 51/100\n",
            "455/455 [==============================] - 0s 183us/step - loss: 40.5359 - mean_absolute_error: 4.7701\n",
            "Epoch 52/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 41.2368 - mean_absolute_error: 4.8772\n",
            "Epoch 53/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 39.8937 - mean_absolute_error: 4.7368\n",
            "Epoch 54/100\n",
            "455/455 [==============================] - 0s 187us/step - loss: 39.2545 - mean_absolute_error: 4.6433\n",
            "Epoch 55/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 38.5007 - mean_absolute_error: 4.5658\n",
            "Epoch 56/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 38.9034 - mean_absolute_error: 4.6813\n",
            "Epoch 57/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 40.4467 - mean_absolute_error: 4.8034\n",
            "Epoch 58/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 38.1153 - mean_absolute_error: 4.5938\n",
            "Epoch 59/100\n",
            "455/455 [==============================] - 0s 187us/step - loss: 38.9267 - mean_absolute_error: 4.6108\n",
            "Epoch 60/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 39.2941 - mean_absolute_error: 4.6744\n",
            "Epoch 61/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 39.0668 - mean_absolute_error: 4.6905\n",
            "Epoch 62/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 41.5887 - mean_absolute_error: 4.8234\n",
            "Epoch 63/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 38.8894 - mean_absolute_error: 4.6370\n",
            "Epoch 64/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 37.9943 - mean_absolute_error: 4.5479\n",
            "Epoch 65/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 37.8792 - mean_absolute_error: 4.6280\n",
            "Epoch 66/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 38.6169 - mean_absolute_error: 4.5476\n",
            "Epoch 67/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 38.3407 - mean_absolute_error: 4.6108\n",
            "Epoch 68/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 38.5828 - mean_absolute_error: 4.6317\n",
            "Epoch 69/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 39.3068 - mean_absolute_error: 4.7193\n",
            "Epoch 70/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 39.4303 - mean_absolute_error: 4.8697\n",
            "Epoch 71/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 37.5986 - mean_absolute_error: 4.5574\n",
            "Epoch 72/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 36.6750 - mean_absolute_error: 4.4780\n",
            "Epoch 73/100\n",
            "455/455 [==============================] - 0s 185us/step - loss: 36.1651 - mean_absolute_error: 4.4127\n",
            "Epoch 74/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 35.7795 - mean_absolute_error: 4.4503\n",
            "Epoch 75/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 37.4444 - mean_absolute_error: 4.5487\n",
            "Epoch 76/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 36.9027 - mean_absolute_error: 4.5665\n",
            "Epoch 77/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 35.4661 - mean_absolute_error: 4.4584\n",
            "Epoch 78/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 37.2620 - mean_absolute_error: 4.5708\n",
            "Epoch 79/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 35.3191 - mean_absolute_error: 4.5197\n",
            "Epoch 80/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 35.5592 - mean_absolute_error: 4.4288\n",
            "Epoch 81/100\n",
            "455/455 [==============================] - 0s 185us/step - loss: 36.6231 - mean_absolute_error: 4.5493\n",
            "Epoch 82/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 39.0643 - mean_absolute_error: 4.7104\n",
            "Epoch 83/100\n",
            "455/455 [==============================] - 0s 184us/step - loss: 39.6474 - mean_absolute_error: 4.7504\n",
            "Epoch 84/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 36.1442 - mean_absolute_error: 4.5268\n",
            "Epoch 85/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 34.7994 - mean_absolute_error: 4.3745\n",
            "Epoch 86/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 35.0079 - mean_absolute_error: 4.3622\n",
            "Epoch 87/100\n",
            "455/455 [==============================] - 0s 529us/step - loss: 34.3904 - mean_absolute_error: 4.3790\n",
            "Epoch 88/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 34.4043 - mean_absolute_error: 4.3577\n",
            "Epoch 89/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 34.1386 - mean_absolute_error: 4.3204\n",
            "Epoch 90/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 34.1329 - mean_absolute_error: 4.2490\n",
            "Epoch 91/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 37.2987 - mean_absolute_error: 4.6090\n",
            "Epoch 92/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 35.5796 - mean_absolute_error: 4.4175\n",
            "Epoch 93/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 38.6665 - mean_absolute_error: 4.7609\n",
            "Epoch 94/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 38.5960 - mean_absolute_error: 4.6074\n",
            "Epoch 95/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 34.8005 - mean_absolute_error: 4.3429\n",
            "Epoch 96/100\n",
            "455/455 [==============================] - 0s 187us/step - loss: 37.2176 - mean_absolute_error: 4.5739\n",
            "Epoch 97/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 37.5237 - mean_absolute_error: 4.6129\n",
            "Epoch 98/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 36.1936 - mean_absolute_error: 4.4958\n",
            "Epoch 99/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 34.0337 - mean_absolute_error: 4.2399\n",
            "Epoch 100/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 36.1595 - mean_absolute_error: 4.5205\n",
            "51/51 [==============================] - 0s 2ms/step\n",
            "Epoch 1/100\n",
            "455/455 [==============================] - 0s 849us/step - loss: 1014.3663 - mean_absolute_error: 22.8430\n",
            "Epoch 2/100\n",
            "455/455 [==============================] - 0s 181us/step - loss: 326.1985 - mean_absolute_error: 14.6060\n",
            "Epoch 3/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 218.2738 - mean_absolute_error: 12.0590\n",
            "Epoch 4/100\n",
            "455/455 [==============================] - 0s 257us/step - loss: 150.6835 - mean_absolute_error: 9.7542\n",
            "Epoch 5/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 99.8928 - mean_absolute_error: 7.7252\n",
            "Epoch 6/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 79.7142 - mean_absolute_error: 6.6832\n",
            "Epoch 7/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 66.8607 - mean_absolute_error: 5.9063\n",
            "Epoch 8/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 59.1898 - mean_absolute_error: 5.5253\n",
            "Epoch 9/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 57.1464 - mean_absolute_error: 5.4618\n",
            "Epoch 10/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 57.4010 - mean_absolute_error: 5.5874\n",
            "Epoch 11/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 51.9850 - mean_absolute_error: 5.1106\n",
            "Epoch 12/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 47.3240 - mean_absolute_error: 4.9239\n",
            "Epoch 13/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 45.0548 - mean_absolute_error: 4.7073\n",
            "Epoch 14/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 42.6459 - mean_absolute_error: 4.5419\n",
            "Epoch 15/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 40.9514 - mean_absolute_error: 4.3238\n",
            "Epoch 16/100\n",
            "455/455 [==============================] - 0s 225us/step - loss: 40.3917 - mean_absolute_error: 4.3830\n",
            "Epoch 17/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 40.2093 - mean_absolute_error: 4.3938\n",
            "Epoch 18/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 39.7517 - mean_absolute_error: 4.4910\n",
            "Epoch 19/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 38.3402 - mean_absolute_error: 4.2181\n",
            "Epoch 20/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 36.2016 - mean_absolute_error: 4.2215\n",
            "Epoch 21/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 37.2900 - mean_absolute_error: 4.2466\n",
            "Epoch 22/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 37.4211 - mean_absolute_error: 4.3080\n",
            "Epoch 23/100\n",
            "455/455 [==============================] - 0s 221us/step - loss: 38.7775 - mean_absolute_error: 4.3996\n",
            "Epoch 24/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 33.8713 - mean_absolute_error: 4.1466\n",
            "Epoch 25/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 34.9917 - mean_absolute_error: 4.1855\n",
            "Epoch 26/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 33.5386 - mean_absolute_error: 4.0919\n",
            "Epoch 27/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 33.2044 - mean_absolute_error: 4.0281\n",
            "Epoch 28/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 32.9052 - mean_absolute_error: 4.0240\n",
            "Epoch 29/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 33.3123 - mean_absolute_error: 3.9879\n",
            "Epoch 30/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 35.3409 - mean_absolute_error: 4.2825\n",
            "Epoch 31/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 33.8162 - mean_absolute_error: 4.1231\n",
            "Epoch 32/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 33.7006 - mean_absolute_error: 4.1366\n",
            "Epoch 33/100\n",
            "455/455 [==============================] - 0s 219us/step - loss: 31.9434 - mean_absolute_error: 4.0565\n",
            "Epoch 34/100\n",
            "455/455 [==============================] - 0s 218us/step - loss: 31.7005 - mean_absolute_error: 3.9249\n",
            "Epoch 35/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 32.7011 - mean_absolute_error: 4.1021\n",
            "Epoch 36/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 33.0921 - mean_absolute_error: 4.1322\n",
            "Epoch 37/100\n",
            "455/455 [==============================] - 0s 214us/step - loss: 33.0163 - mean_absolute_error: 4.0473\n",
            "Epoch 38/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 31.7429 - mean_absolute_error: 4.0414\n",
            "Epoch 39/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 31.5727 - mean_absolute_error: 4.0251\n",
            "Epoch 40/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 31.9549 - mean_absolute_error: 3.9306\n",
            "Epoch 41/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 31.4654 - mean_absolute_error: 3.9627\n",
            "Epoch 42/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 29.9090 - mean_absolute_error: 3.8522\n",
            "Epoch 43/100\n",
            "455/455 [==============================] - 0s 187us/step - loss: 31.8530 - mean_absolute_error: 4.0346\n",
            "Epoch 44/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 31.3145 - mean_absolute_error: 3.9448\n",
            "Epoch 45/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 34.7012 - mean_absolute_error: 4.2707\n",
            "Epoch 46/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 30.1693 - mean_absolute_error: 3.9037\n",
            "Epoch 47/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 33.6733 - mean_absolute_error: 4.2437\n",
            "Epoch 48/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 30.7369 - mean_absolute_error: 3.9566\n",
            "Epoch 49/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 32.1388 - mean_absolute_error: 4.1036\n",
            "Epoch 50/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 31.6420 - mean_absolute_error: 3.9338\n",
            "Epoch 51/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 30.3443 - mean_absolute_error: 3.9327\n",
            "Epoch 52/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 39.3987 - mean_absolute_error: 4.5373\n",
            "Epoch 53/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 30.1400 - mean_absolute_error: 3.9062\n",
            "Epoch 54/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 29.3450 - mean_absolute_error: 3.8148\n",
            "Epoch 55/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 31.1252 - mean_absolute_error: 3.9505\n",
            "Epoch 56/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 30.4406 - mean_absolute_error: 3.8939\n",
            "Epoch 57/100\n",
            "455/455 [==============================] - 0s 218us/step - loss: 32.1841 - mean_absolute_error: 4.0397\n",
            "Epoch 58/100\n",
            "455/455 [==============================] - 0s 217us/step - loss: 30.3316 - mean_absolute_error: 3.8638\n",
            "Epoch 59/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 29.3653 - mean_absolute_error: 3.8536\n",
            "Epoch 60/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 29.8003 - mean_absolute_error: 3.8492\n",
            "Epoch 61/100\n",
            "455/455 [==============================] - 0s 225us/step - loss: 30.8950 - mean_absolute_error: 3.9369\n",
            "Epoch 62/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 32.4250 - mean_absolute_error: 4.1806\n",
            "Epoch 63/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 30.2732 - mean_absolute_error: 3.8655\n",
            "Epoch 64/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 30.7705 - mean_absolute_error: 4.0319\n",
            "Epoch 65/100\n",
            "455/455 [==============================] - 0s 217us/step - loss: 30.8473 - mean_absolute_error: 3.9736\n",
            "Epoch 66/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 28.3341 - mean_absolute_error: 3.7608\n",
            "Epoch 67/100\n",
            "455/455 [==============================] - 0s 213us/step - loss: 29.5922 - mean_absolute_error: 3.9589\n",
            "Epoch 68/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 32.3404 - mean_absolute_error: 4.1944\n",
            "Epoch 69/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 29.8754 - mean_absolute_error: 3.9130\n",
            "Epoch 70/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 28.7303 - mean_absolute_error: 3.8039\n",
            "Epoch 71/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 31.0709 - mean_absolute_error: 3.9696\n",
            "Epoch 72/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 28.5959 - mean_absolute_error: 3.7528\n",
            "Epoch 73/100\n",
            "455/455 [==============================] - 0s 221us/step - loss: 30.0737 - mean_absolute_error: 3.8771\n",
            "Epoch 74/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 27.7825 - mean_absolute_error: 3.6945\n",
            "Epoch 75/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 31.7345 - mean_absolute_error: 4.0275\n",
            "Epoch 76/100\n",
            "455/455 [==============================] - 0s 223us/step - loss: 28.8396 - mean_absolute_error: 3.8863\n",
            "Epoch 77/100\n",
            "455/455 [==============================] - 0s 218us/step - loss: 32.4961 - mean_absolute_error: 4.1189\n",
            "Epoch 78/100\n",
            "455/455 [==============================] - 0s 221us/step - loss: 29.3958 - mean_absolute_error: 3.9597\n",
            "Epoch 79/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 29.1342 - mean_absolute_error: 3.8958\n",
            "Epoch 80/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 28.3163 - mean_absolute_error: 3.8001\n",
            "Epoch 81/100\n",
            "455/455 [==============================] - 0s 187us/step - loss: 29.2313 - mean_absolute_error: 3.7861\n",
            "Epoch 82/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 28.6146 - mean_absolute_error: 3.8773\n",
            "Epoch 83/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 29.4274 - mean_absolute_error: 3.8930\n",
            "Epoch 84/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 28.4807 - mean_absolute_error: 3.6809\n",
            "Epoch 85/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 27.5079 - mean_absolute_error: 3.8185\n",
            "Epoch 86/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 28.5688 - mean_absolute_error: 3.7959\n",
            "Epoch 87/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 28.1626 - mean_absolute_error: 3.7608\n",
            "Epoch 88/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 28.9056 - mean_absolute_error: 3.8485\n",
            "Epoch 89/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 27.0239 - mean_absolute_error: 3.6701\n",
            "Epoch 90/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 27.9457 - mean_absolute_error: 3.6914\n",
            "Epoch 91/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 27.7252 - mean_absolute_error: 3.6615\n",
            "Epoch 92/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 29.7974 - mean_absolute_error: 3.9025\n",
            "Epoch 93/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 30.2912 - mean_absolute_error: 4.0010\n",
            "Epoch 94/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 27.8424 - mean_absolute_error: 3.6850\n",
            "Epoch 95/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 27.3892 - mean_absolute_error: 3.6487\n",
            "Epoch 96/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 27.1649 - mean_absolute_error: 3.7218\n",
            "Epoch 97/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 27.0174 - mean_absolute_error: 3.6425\n",
            "Epoch 98/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 29.4275 - mean_absolute_error: 3.8628\n",
            "Epoch 99/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 27.3128 - mean_absolute_error: 3.6231\n",
            "Epoch 100/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 26.6343 - mean_absolute_error: 3.7287\n",
            "51/51 [==============================] - 0s 3ms/step\n",
            "Epoch 1/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 47219.8374 - mean_absolute_error: 195.9168\n",
            "Epoch 2/100\n",
            "455/455 [==============================] - 0s 185us/step - loss: 2151.3328 - mean_absolute_error: 37.6768\n",
            "Epoch 3/100\n",
            "455/455 [==============================] - 0s 225us/step - loss: 267.6217 - mean_absolute_error: 12.1710\n",
            "Epoch 4/100\n",
            "455/455 [==============================] - 0s 213us/step - loss: 240.5558 - mean_absolute_error: 11.7376\n",
            "Epoch 5/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 218.0223 - mean_absolute_error: 11.1887\n",
            "Epoch 6/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 198.0348 - mean_absolute_error: 10.6689\n",
            "Epoch 7/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 181.3311 - mean_absolute_error: 10.3031\n",
            "Epoch 8/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 165.9997 - mean_absolute_error: 9.8080\n",
            "Epoch 9/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 153.1666 - mean_absolute_error: 9.5737\n",
            "Epoch 10/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 143.1143 - mean_absolute_error: 9.2057\n",
            "Epoch 11/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 133.9652 - mean_absolute_error: 8.9832\n",
            "Epoch 12/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 126.2559 - mean_absolute_error: 8.7052\n",
            "Epoch 13/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 121.4130 - mean_absolute_error: 8.5224\n",
            "Epoch 14/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 116.3259 - mean_absolute_error: 8.2780\n",
            "Epoch 15/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 110.5682 - mean_absolute_error: 8.1947\n",
            "Epoch 16/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 106.5432 - mean_absolute_error: 7.9954\n",
            "Epoch 17/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 104.7479 - mean_absolute_error: 7.8993\n",
            "Epoch 18/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 100.5384 - mean_absolute_error: 7.7452\n",
            "Epoch 19/100\n",
            "455/455 [==============================] - 0s 214us/step - loss: 96.5762 - mean_absolute_error: 7.5908\n",
            "Epoch 20/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 94.7502 - mean_absolute_error: 7.5382\n",
            "Epoch 21/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 91.4946 - mean_absolute_error: 7.3619\n",
            "Epoch 22/100\n",
            "455/455 [==============================] - 0s 229us/step - loss: 88.5225 - mean_absolute_error: 7.2992\n",
            "Epoch 23/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 86.2455 - mean_absolute_error: 7.2127\n",
            "Epoch 24/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 83.5900 - mean_absolute_error: 7.0665\n",
            "Epoch 25/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 82.1753 - mean_absolute_error: 7.0079\n",
            "Epoch 26/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 78.0302 - mean_absolute_error: 6.8248\n",
            "Epoch 27/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 78.0341 - mean_absolute_error: 6.8285\n",
            "Epoch 28/100\n",
            "455/455 [==============================] - 0s 187us/step - loss: 73.1870 - mean_absolute_error: 6.6021\n",
            "Epoch 29/100\n",
            "455/455 [==============================] - 0s 214us/step - loss: 70.4089 - mean_absolute_error: 6.5288\n",
            "Epoch 30/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 67.7941 - mean_absolute_error: 6.2939\n",
            "Epoch 31/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 65.1510 - mean_absolute_error: 6.1530\n",
            "Epoch 32/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 62.4612 - mean_absolute_error: 6.0199\n",
            "Epoch 33/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 61.2753 - mean_absolute_error: 5.9322\n",
            "Epoch 34/100\n",
            "455/455 [==============================] - 0s 188us/step - loss: 58.4628 - mean_absolute_error: 5.7616\n",
            "Epoch 35/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 56.8272 - mean_absolute_error: 5.6657\n",
            "Epoch 36/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 54.0062 - mean_absolute_error: 5.5433\n",
            "Epoch 37/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 51.1159 - mean_absolute_error: 5.3108\n",
            "Epoch 38/100\n",
            "455/455 [==============================] - 0s 184us/step - loss: 48.8125 - mean_absolute_error: 5.0727\n",
            "Epoch 39/100\n",
            "455/455 [==============================] - 0s 209us/step - loss: 46.7499 - mean_absolute_error: 4.9565\n",
            "Epoch 40/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 46.0354 - mean_absolute_error: 4.9983\n",
            "Epoch 41/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 46.0179 - mean_absolute_error: 4.9598\n",
            "Epoch 42/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 43.0229 - mean_absolute_error: 4.7850\n",
            "Epoch 43/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 42.5758 - mean_absolute_error: 4.7320\n",
            "Epoch 44/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 41.2961 - mean_absolute_error: 4.6721\n",
            "Epoch 45/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 41.4713 - mean_absolute_error: 4.6796\n",
            "Epoch 46/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 40.1710 - mean_absolute_error: 4.6010\n",
            "Epoch 47/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 40.2388 - mean_absolute_error: 4.6495\n",
            "Epoch 48/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 39.3255 - mean_absolute_error: 4.5211\n",
            "Epoch 49/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 39.2920 - mean_absolute_error: 4.5912\n",
            "Epoch 50/100\n",
            "455/455 [==============================] - 0s 218us/step - loss: 39.0040 - mean_absolute_error: 4.5685\n",
            "Epoch 51/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 38.6487 - mean_absolute_error: 4.5009\n",
            "Epoch 52/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 39.4426 - mean_absolute_error: 4.5657\n",
            "Epoch 53/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 38.6047 - mean_absolute_error: 4.4918\n",
            "Epoch 54/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 40.2229 - mean_absolute_error: 4.6465\n",
            "Epoch 55/100\n",
            "455/455 [==============================] - 0s 193us/step - loss: 39.2136 - mean_absolute_error: 4.6676\n",
            "Epoch 56/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 38.5220 - mean_absolute_error: 4.6075\n",
            "Epoch 57/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 37.8266 - mean_absolute_error: 4.5383\n",
            "Epoch 58/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 38.4946 - mean_absolute_error: 4.5415\n",
            "Epoch 59/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 38.5775 - mean_absolute_error: 4.6748\n",
            "Epoch 60/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 38.8458 - mean_absolute_error: 4.5723\n",
            "Epoch 61/100\n",
            "455/455 [==============================] - 0s 222us/step - loss: 36.0726 - mean_absolute_error: 4.4392\n",
            "Epoch 62/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 40.7007 - mean_absolute_error: 4.7797\n",
            "Epoch 63/100\n",
            "455/455 [==============================] - 0s 213us/step - loss: 36.2289 - mean_absolute_error: 4.4808\n",
            "Epoch 64/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 36.2867 - mean_absolute_error: 4.4801\n",
            "Epoch 65/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 37.1828 - mean_absolute_error: 4.4685\n",
            "Epoch 66/100\n",
            "455/455 [==============================] - 0s 215us/step - loss: 36.3535 - mean_absolute_error: 4.5640\n",
            "Epoch 67/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 38.7241 - mean_absolute_error: 4.5281\n",
            "Epoch 68/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 38.1819 - mean_absolute_error: 4.5496\n",
            "Epoch 69/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 39.1708 - mean_absolute_error: 4.6128\n",
            "Epoch 70/100\n",
            "455/455 [==============================] - 0s 213us/step - loss: 39.6451 - mean_absolute_error: 4.7624\n",
            "Epoch 71/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 37.8381 - mean_absolute_error: 4.5597\n",
            "Epoch 72/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 36.4278 - mean_absolute_error: 4.4604\n",
            "Epoch 73/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 35.7733 - mean_absolute_error: 4.5193\n",
            "Epoch 74/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 35.6985 - mean_absolute_error: 4.3702\n",
            "Epoch 75/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 36.8695 - mean_absolute_error: 4.5192\n",
            "Epoch 76/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 35.5291 - mean_absolute_error: 4.4418\n",
            "Epoch 77/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 35.6625 - mean_absolute_error: 4.4789\n",
            "Epoch 78/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 36.9848 - mean_absolute_error: 4.5566\n",
            "Epoch 79/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 35.5979 - mean_absolute_error: 4.4403\n",
            "Epoch 80/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 36.0572 - mean_absolute_error: 4.4844\n",
            "Epoch 81/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 36.7328 - mean_absolute_error: 4.5535\n",
            "Epoch 82/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 34.6224 - mean_absolute_error: 4.3247\n",
            "Epoch 83/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 35.3715 - mean_absolute_error: 4.4261\n",
            "Epoch 84/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 37.0991 - mean_absolute_error: 4.5432\n",
            "Epoch 85/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 36.1727 - mean_absolute_error: 4.4293\n",
            "Epoch 86/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 35.5420 - mean_absolute_error: 4.4473\n",
            "Epoch 87/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 33.4509 - mean_absolute_error: 4.2973\n",
            "Epoch 88/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 35.4929 - mean_absolute_error: 4.3932\n",
            "Epoch 89/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 35.6203 - mean_absolute_error: 4.5203\n",
            "Epoch 90/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 33.9793 - mean_absolute_error: 4.3593\n",
            "Epoch 91/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 39.9037 - mean_absolute_error: 4.7566\n",
            "Epoch 92/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 34.7098 - mean_absolute_error: 4.4145\n",
            "Epoch 93/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 35.0538 - mean_absolute_error: 4.3691\n",
            "Epoch 94/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 33.7341 - mean_absolute_error: 4.2995\n",
            "Epoch 95/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 35.3764 - mean_absolute_error: 4.5131\n",
            "Epoch 96/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 39.0863 - mean_absolute_error: 4.7752\n",
            "Epoch 97/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 34.3492 - mean_absolute_error: 4.2919\n",
            "Epoch 98/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 37.0609 - mean_absolute_error: 4.4623\n",
            "Epoch 99/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 33.5312 - mean_absolute_error: 4.3210\n",
            "Epoch 100/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 33.6615 - mean_absolute_error: 4.2361\n",
            "51/51 [==============================] - 0s 3ms/step\n",
            "Epoch 1/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 2088.5413 - mean_absolute_error: 40.5305\n",
            "Epoch 2/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 425.2546 - mean_absolute_error: 17.5484\n",
            "Epoch 3/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 99.9885 - mean_absolute_error: 7.6119\n",
            "Epoch 4/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 79.1299 - mean_absolute_error: 6.4574\n",
            "Epoch 5/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 72.5293 - mean_absolute_error: 6.2174\n",
            "Epoch 6/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 68.1677 - mean_absolute_error: 5.9394\n",
            "Epoch 7/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 66.9145 - mean_absolute_error: 5.8242\n",
            "Epoch 8/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 64.5377 - mean_absolute_error: 5.6932\n",
            "Epoch 9/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 61.3577 - mean_absolute_error: 5.5558\n",
            "Epoch 10/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 60.2354 - mean_absolute_error: 5.4844\n",
            "Epoch 11/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 58.8117 - mean_absolute_error: 5.3854\n",
            "Epoch 12/100\n",
            "455/455 [==============================] - 0s 209us/step - loss: 56.5525 - mean_absolute_error: 5.2968\n",
            "Epoch 13/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 53.7078 - mean_absolute_error: 5.0496\n",
            "Epoch 14/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 52.9407 - mean_absolute_error: 5.0980\n",
            "Epoch 15/100\n",
            "455/455 [==============================] - 0s 215us/step - loss: 51.5293 - mean_absolute_error: 4.9328\n",
            "Epoch 16/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 50.1705 - mean_absolute_error: 4.8900\n",
            "Epoch 17/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 48.8619 - mean_absolute_error: 4.7953\n",
            "Epoch 18/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 46.7125 - mean_absolute_error: 4.7235\n",
            "Epoch 19/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 45.9221 - mean_absolute_error: 4.7900\n",
            "Epoch 20/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 47.0654 - mean_absolute_error: 4.8218\n",
            "Epoch 21/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 45.6743 - mean_absolute_error: 4.7134\n",
            "Epoch 22/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 44.7946 - mean_absolute_error: 4.7167\n",
            "Epoch 23/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 42.5104 - mean_absolute_error: 4.5410\n",
            "Epoch 24/100\n",
            "455/455 [==============================] - 0s 203us/step - loss: 42.9980 - mean_absolute_error: 4.6532\n",
            "Epoch 25/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 45.1724 - mean_absolute_error: 4.7118\n",
            "Epoch 26/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 40.7207 - mean_absolute_error: 4.4095\n",
            "Epoch 27/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 40.3805 - mean_absolute_error: 4.3930\n",
            "Epoch 28/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 39.0855 - mean_absolute_error: 4.3236\n",
            "Epoch 29/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 39.3620 - mean_absolute_error: 4.3322\n",
            "Epoch 30/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 39.4367 - mean_absolute_error: 4.3821\n",
            "Epoch 31/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 40.2628 - mean_absolute_error: 4.4773\n",
            "Epoch 32/100\n",
            "455/455 [==============================] - 0s 213us/step - loss: 37.9116 - mean_absolute_error: 4.2778\n",
            "Epoch 33/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 38.9898 - mean_absolute_error: 4.4204\n",
            "Epoch 34/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 37.9555 - mean_absolute_error: 4.5651\n",
            "Epoch 35/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 37.7844 - mean_absolute_error: 4.2921\n",
            "Epoch 36/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 38.5375 - mean_absolute_error: 4.4072\n",
            "Epoch 37/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 38.1055 - mean_absolute_error: 4.3847\n",
            "Epoch 38/100\n",
            "455/455 [==============================] - 0s 215us/step - loss: 36.7461 - mean_absolute_error: 4.3495\n",
            "Epoch 39/100\n",
            "455/455 [==============================] - 0s 212us/step - loss: 37.3256 - mean_absolute_error: 4.3445\n",
            "Epoch 40/100\n",
            "455/455 [==============================] - 0s 217us/step - loss: 36.7336 - mean_absolute_error: 4.2943\n",
            "Epoch 41/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 35.2689 - mean_absolute_error: 4.1979\n",
            "Epoch 42/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 34.8853 - mean_absolute_error: 4.2033\n",
            "Epoch 43/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 36.1693 - mean_absolute_error: 4.2330\n",
            "Epoch 44/100\n",
            "455/455 [==============================] - 0s 206us/step - loss: 35.9646 - mean_absolute_error: 4.3308\n",
            "Epoch 45/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 36.8554 - mean_absolute_error: 4.2994\n",
            "Epoch 46/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 34.8410 - mean_absolute_error: 4.2894\n",
            "Epoch 47/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 34.9669 - mean_absolute_error: 4.1931\n",
            "Epoch 48/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 35.0206 - mean_absolute_error: 4.1652\n",
            "Epoch 49/100\n",
            "455/455 [==============================] - 0s 216us/step - loss: 34.2477 - mean_absolute_error: 4.2202\n",
            "Epoch 50/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 34.4289 - mean_absolute_error: 4.1987\n",
            "Epoch 51/100\n",
            "455/455 [==============================] - 0s 194us/step - loss: 34.4272 - mean_absolute_error: 4.1862\n",
            "Epoch 52/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 35.4033 - mean_absolute_error: 4.2542\n",
            "Epoch 53/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 33.5813 - mean_absolute_error: 4.2483\n",
            "Epoch 54/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 33.8393 - mean_absolute_error: 4.1539\n",
            "Epoch 55/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 33.7799 - mean_absolute_error: 4.1863\n",
            "Epoch 56/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 34.0602 - mean_absolute_error: 4.1882\n",
            "Epoch 57/100\n",
            "455/455 [==============================] - 0s 190us/step - loss: 32.6622 - mean_absolute_error: 4.0897\n",
            "Epoch 58/100\n",
            "455/455 [==============================] - 0s 189us/step - loss: 34.0044 - mean_absolute_error: 4.1477\n",
            "Epoch 59/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 35.8297 - mean_absolute_error: 4.3499\n",
            "Epoch 60/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 33.1760 - mean_absolute_error: 4.1377\n",
            "Epoch 61/100\n",
            "455/455 [==============================] - 0s 192us/step - loss: 32.8312 - mean_absolute_error: 4.1314\n",
            "Epoch 62/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 36.4613 - mean_absolute_error: 4.3091\n",
            "Epoch 63/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 32.6179 - mean_absolute_error: 4.3112\n",
            "Epoch 64/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 32.9633 - mean_absolute_error: 4.1749\n",
            "Epoch 65/100\n",
            "455/455 [==============================] - 0s 191us/step - loss: 32.0149 - mean_absolute_error: 4.1107\n",
            "Epoch 66/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 34.0258 - mean_absolute_error: 4.1784\n",
            "Epoch 67/100\n",
            "455/455 [==============================] - 0s 223us/step - loss: 34.1767 - mean_absolute_error: 4.2783\n",
            "Epoch 68/100\n",
            "455/455 [==============================] - 0s 213us/step - loss: 34.6350 - mean_absolute_error: 4.3395\n",
            "Epoch 69/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 32.7178 - mean_absolute_error: 4.1388\n",
            "Epoch 70/100\n",
            "455/455 [==============================] - 0s 217us/step - loss: 32.8936 - mean_absolute_error: 4.1387\n",
            "Epoch 71/100\n",
            "455/455 [==============================] - 0s 227us/step - loss: 31.8522 - mean_absolute_error: 4.0933\n",
            "Epoch 72/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 32.4809 - mean_absolute_error: 4.1314\n",
            "Epoch 73/100\n",
            "455/455 [==============================] - 0s 208us/step - loss: 32.4051 - mean_absolute_error: 4.1415\n",
            "Epoch 74/100\n",
            "455/455 [==============================] - 0s 211us/step - loss: 32.1478 - mean_absolute_error: 4.0924\n",
            "Epoch 75/100\n",
            "455/455 [==============================] - 0s 210us/step - loss: 32.4890 - mean_absolute_error: 4.1839\n",
            "Epoch 76/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 34.3118 - mean_absolute_error: 4.2545\n",
            "Epoch 77/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 35.2490 - mean_absolute_error: 4.3512\n",
            "Epoch 78/100\n",
            "455/455 [==============================] - 0s 195us/step - loss: 31.9719 - mean_absolute_error: 4.0465\n",
            "Epoch 79/100\n",
            "455/455 [==============================] - 0s 196us/step - loss: 33.6829 - mean_absolute_error: 4.3086\n",
            "Epoch 80/100\n",
            "455/455 [==============================] - 0s 218us/step - loss: 33.2977 - mean_absolute_error: 4.2546\n",
            "Epoch 81/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 31.6525 - mean_absolute_error: 4.0931\n",
            "Epoch 82/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 31.2683 - mean_absolute_error: 3.9828\n",
            "Epoch 83/100\n",
            "455/455 [==============================] - 0s 204us/step - loss: 31.4328 - mean_absolute_error: 4.1076\n",
            "Epoch 84/100\n",
            "455/455 [==============================] - 0s 207us/step - loss: 31.6842 - mean_absolute_error: 4.0526\n",
            "Epoch 85/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 30.7696 - mean_absolute_error: 4.0332\n",
            "Epoch 86/100\n",
            "455/455 [==============================] - 0s 197us/step - loss: 31.3467 - mean_absolute_error: 4.0162\n",
            "Epoch 87/100\n",
            "455/455 [==============================] - 0s 209us/step - loss: 30.7567 - mean_absolute_error: 3.9586\n",
            "Epoch 88/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 33.0936 - mean_absolute_error: 4.2021\n",
            "Epoch 89/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 31.5380 - mean_absolute_error: 4.0893\n",
            "Epoch 90/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 30.8253 - mean_absolute_error: 4.0000\n",
            "Epoch 91/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 31.0313 - mean_absolute_error: 4.0003\n",
            "Epoch 92/100\n",
            "455/455 [==============================] - 0s 198us/step - loss: 29.8928 - mean_absolute_error: 4.0031\n",
            "Epoch 93/100\n",
            "455/455 [==============================] - 0s 215us/step - loss: 31.3342 - mean_absolute_error: 4.0520\n",
            "Epoch 94/100\n",
            "455/455 [==============================] - 0s 205us/step - loss: 31.0771 - mean_absolute_error: 4.0131\n",
            "Epoch 95/100\n",
            "455/455 [==============================] - 0s 209us/step - loss: 31.2147 - mean_absolute_error: 3.9926\n",
            "Epoch 96/100\n",
            "455/455 [==============================] - 0s 201us/step - loss: 30.5156 - mean_absolute_error: 3.9551\n",
            "Epoch 97/100\n",
            "455/455 [==============================] - 0s 202us/step - loss: 29.6849 - mean_absolute_error: 3.9201\n",
            "Epoch 98/100\n",
            "455/455 [==============================] - 0s 199us/step - loss: 29.8740 - mean_absolute_error: 3.9319\n",
            "Epoch 99/100\n",
            "455/455 [==============================] - 0s 200us/step - loss: 32.5548 - mean_absolute_error: 4.1003\n",
            "Epoch 100/100\n",
            "455/455 [==============================] - 0s 209us/step - loss: 32.0738 - mean_absolute_error: 4.1386\n",
            "51/51 [==============================] - 0s 3ms/step\n",
            "Epoch 1/100\n",
            "456/456 [==============================] - 1s 1ms/step - loss: 285.4318 - mean_absolute_error: 12.0834\n",
            "Epoch 2/100\n",
            "456/456 [==============================] - 0s 197us/step - loss: 108.0941 - mean_absolute_error: 7.4992\n",
            "Epoch 3/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 102.3899 - mean_absolute_error: 7.3068\n",
            "Epoch 4/100\n",
            "456/456 [==============================] - 0s 194us/step - loss: 94.2066 - mean_absolute_error: 6.9374\n",
            "Epoch 5/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 86.5531 - mean_absolute_error: 6.4671\n",
            "Epoch 6/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 82.0827 - mean_absolute_error: 6.3204\n",
            "Epoch 7/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 76.5306 - mean_absolute_error: 6.0940\n",
            "Epoch 8/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 73.7841 - mean_absolute_error: 6.0283\n",
            "Epoch 9/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 67.6486 - mean_absolute_error: 5.7683\n",
            "Epoch 10/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 66.7917 - mean_absolute_error: 5.7908\n",
            "Epoch 11/100\n",
            "456/456 [==============================] - 0s 198us/step - loss: 61.9729 - mean_absolute_error: 5.4375\n",
            "Epoch 12/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 60.3350 - mean_absolute_error: 5.4900\n",
            "Epoch 13/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 54.8890 - mean_absolute_error: 5.1794\n",
            "Epoch 14/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 53.6901 - mean_absolute_error: 5.0568\n",
            "Epoch 15/100\n",
            "456/456 [==============================] - 0s 199us/step - loss: 50.7339 - mean_absolute_error: 5.0752\n",
            "Epoch 16/100\n",
            "456/456 [==============================] - 0s 199us/step - loss: 48.9496 - mean_absolute_error: 4.9851\n",
            "Epoch 17/100\n",
            "456/456 [==============================] - 0s 194us/step - loss: 48.2877 - mean_absolute_error: 4.9091\n",
            "Epoch 18/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 44.6225 - mean_absolute_error: 4.6099\n",
            "Epoch 19/100\n",
            "456/456 [==============================] - 0s 194us/step - loss: 45.0024 - mean_absolute_error: 4.7035\n",
            "Epoch 20/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 44.4486 - mean_absolute_error: 4.8256\n",
            "Epoch 21/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 39.3929 - mean_absolute_error: 4.5039\n",
            "Epoch 22/100\n",
            "456/456 [==============================] - 0s 195us/step - loss: 41.4666 - mean_absolute_error: 4.6306\n",
            "Epoch 23/100\n",
            "456/456 [==============================] - 0s 197us/step - loss: 38.0837 - mean_absolute_error: 4.5128\n",
            "Epoch 24/100\n",
            "456/456 [==============================] - 0s 198us/step - loss: 38.9383 - mean_absolute_error: 4.5064\n",
            "Epoch 25/100\n",
            "456/456 [==============================] - 0s 192us/step - loss: 37.6435 - mean_absolute_error: 4.4137\n",
            "Epoch 26/100\n",
            "456/456 [==============================] - 0s 213us/step - loss: 38.2191 - mean_absolute_error: 4.3662\n",
            "Epoch 27/100\n",
            "456/456 [==============================] - 0s 225us/step - loss: 34.7687 - mean_absolute_error: 4.2135\n",
            "Epoch 28/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 33.5905 - mean_absolute_error: 4.1857\n",
            "Epoch 29/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 33.4179 - mean_absolute_error: 4.1514\n",
            "Epoch 30/100\n",
            "456/456 [==============================] - 0s 216us/step - loss: 37.1826 - mean_absolute_error: 4.5377\n",
            "Epoch 31/100\n",
            "456/456 [==============================] - 0s 218us/step - loss: 35.2146 - mean_absolute_error: 4.2783\n",
            "Epoch 32/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 33.0426 - mean_absolute_error: 4.1124\n",
            "Epoch 33/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 31.4109 - mean_absolute_error: 4.0184\n",
            "Epoch 34/100\n",
            "456/456 [==============================] - 0s 216us/step - loss: 31.9242 - mean_absolute_error: 4.0837\n",
            "Epoch 35/100\n",
            "456/456 [==============================] - 0s 205us/step - loss: 31.8988 - mean_absolute_error: 4.0862\n",
            "Epoch 36/100\n",
            "456/456 [==============================] - 0s 197us/step - loss: 31.6912 - mean_absolute_error: 4.0022\n",
            "Epoch 37/100\n",
            "456/456 [==============================] - 0s 224us/step - loss: 30.0999 - mean_absolute_error: 3.8993\n",
            "Epoch 38/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 30.3049 - mean_absolute_error: 3.9892\n",
            "Epoch 39/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 30.4166 - mean_absolute_error: 3.9600\n",
            "Epoch 40/100\n",
            "456/456 [==============================] - 0s 215us/step - loss: 30.8796 - mean_absolute_error: 3.9958\n",
            "Epoch 41/100\n",
            "456/456 [==============================] - 0s 205us/step - loss: 33.3679 - mean_absolute_error: 4.2457\n",
            "Epoch 42/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 31.1366 - mean_absolute_error: 4.0265\n",
            "Epoch 43/100\n",
            "456/456 [==============================] - 0s 205us/step - loss: 28.9208 - mean_absolute_error: 3.8559\n",
            "Epoch 44/100\n",
            "456/456 [==============================] - 0s 213us/step - loss: 31.3049 - mean_absolute_error: 4.1143\n",
            "Epoch 45/100\n",
            "456/456 [==============================] - 0s 200us/step - loss: 28.3396 - mean_absolute_error: 3.8304\n",
            "Epoch 46/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 29.0453 - mean_absolute_error: 3.9320\n",
            "Epoch 47/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 30.8460 - mean_absolute_error: 3.9903\n",
            "Epoch 48/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 32.1504 - mean_absolute_error: 4.1885\n",
            "Epoch 49/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 30.4417 - mean_absolute_error: 3.9043\n",
            "Epoch 50/100\n",
            "456/456 [==============================] - 0s 197us/step - loss: 31.4563 - mean_absolute_error: 4.0727\n",
            "Epoch 51/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 32.3033 - mean_absolute_error: 4.2093\n",
            "Epoch 52/100\n",
            "456/456 [==============================] - 0s 205us/step - loss: 29.6303 - mean_absolute_error: 3.9402\n",
            "Epoch 53/100\n",
            "456/456 [==============================] - 0s 200us/step - loss: 29.0392 - mean_absolute_error: 3.8177\n",
            "Epoch 54/100\n",
            "456/456 [==============================] - 0s 232us/step - loss: 28.2157 - mean_absolute_error: 3.8534\n",
            "Epoch 55/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 28.9144 - mean_absolute_error: 3.8972\n",
            "Epoch 56/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 29.5448 - mean_absolute_error: 4.0143\n",
            "Epoch 57/100\n",
            "456/456 [==============================] - 0s 200us/step - loss: 28.6180 - mean_absolute_error: 3.7997\n",
            "Epoch 58/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 27.3399 - mean_absolute_error: 3.7749\n",
            "Epoch 59/100\n",
            "456/456 [==============================] - 0s 195us/step - loss: 28.6542 - mean_absolute_error: 3.8540\n",
            "Epoch 60/100\n",
            "456/456 [==============================] - 0s 198us/step - loss: 27.7724 - mean_absolute_error: 3.7982\n",
            "Epoch 61/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 28.7513 - mean_absolute_error: 3.7232\n",
            "Epoch 62/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 30.2307 - mean_absolute_error: 3.9411\n",
            "Epoch 63/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 29.1167 - mean_absolute_error: 3.8593\n",
            "Epoch 64/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 31.0620 - mean_absolute_error: 3.9645\n",
            "Epoch 65/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 27.1449 - mean_absolute_error: 3.7533\n",
            "Epoch 66/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 28.1340 - mean_absolute_error: 3.8890\n",
            "Epoch 67/100\n",
            "456/456 [==============================] - 0s 200us/step - loss: 27.4361 - mean_absolute_error: 3.7204\n",
            "Epoch 68/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 30.4850 - mean_absolute_error: 4.0087\n",
            "Epoch 69/100\n",
            "456/456 [==============================] - 0s 200us/step - loss: 29.9150 - mean_absolute_error: 3.9607\n",
            "Epoch 70/100\n",
            "456/456 [==============================] - 0s 194us/step - loss: 26.4692 - mean_absolute_error: 3.7004\n",
            "Epoch 71/100\n",
            "456/456 [==============================] - 0s 198us/step - loss: 27.2644 - mean_absolute_error: 3.6587\n",
            "Epoch 72/100\n",
            "456/456 [==============================] - 0s 190us/step - loss: 26.6016 - mean_absolute_error: 3.7401\n",
            "Epoch 73/100\n",
            "456/456 [==============================] - 0s 194us/step - loss: 26.9872 - mean_absolute_error: 3.7163\n",
            "Epoch 74/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 27.3213 - mean_absolute_error: 3.8653\n",
            "Epoch 75/100\n",
            "456/456 [==============================] - 0s 197us/step - loss: 27.8264 - mean_absolute_error: 3.8971\n",
            "Epoch 76/100\n",
            "456/456 [==============================] - 0s 191us/step - loss: 27.0571 - mean_absolute_error: 3.6425\n",
            "Epoch 77/100\n",
            "456/456 [==============================] - 0s 194us/step - loss: 27.1228 - mean_absolute_error: 3.6864\n",
            "Epoch 78/100\n",
            "456/456 [==============================] - 0s 238us/step - loss: 28.0017 - mean_absolute_error: 3.8963\n",
            "Epoch 79/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 26.0994 - mean_absolute_error: 3.6509\n",
            "Epoch 80/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 28.5247 - mean_absolute_error: 3.8071\n",
            "Epoch 81/100\n",
            "456/456 [==============================] - 0s 218us/step - loss: 27.4677 - mean_absolute_error: 3.7652\n",
            "Epoch 82/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 27.2270 - mean_absolute_error: 3.8040\n",
            "Epoch 83/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 26.7474 - mean_absolute_error: 3.6788\n",
            "Epoch 84/100\n",
            "456/456 [==============================] - 0s 218us/step - loss: 25.9725 - mean_absolute_error: 3.7562\n",
            "Epoch 85/100\n",
            "456/456 [==============================] - 0s 240us/step - loss: 26.6681 - mean_absolute_error: 3.6649\n",
            "Epoch 86/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 25.9377 - mean_absolute_error: 3.6464\n",
            "Epoch 87/100\n",
            "456/456 [==============================] - 0s 216us/step - loss: 29.4874 - mean_absolute_error: 3.9215\n",
            "Epoch 88/100\n",
            "456/456 [==============================] - 0s 222us/step - loss: 26.5750 - mean_absolute_error: 3.7480\n",
            "Epoch 89/100\n",
            "456/456 [==============================] - 0s 245us/step - loss: 26.5104 - mean_absolute_error: 3.6983\n",
            "Epoch 90/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 26.9894 - mean_absolute_error: 3.7004\n",
            "Epoch 91/100\n",
            "456/456 [==============================] - 0s 222us/step - loss: 28.2013 - mean_absolute_error: 3.7783\n",
            "Epoch 92/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 25.8656 - mean_absolute_error: 3.7065\n",
            "Epoch 93/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 25.6535 - mean_absolute_error: 3.5895\n",
            "Epoch 94/100\n",
            "456/456 [==============================] - 0s 225us/step - loss: 26.6774 - mean_absolute_error: 3.7444\n",
            "Epoch 95/100\n",
            "456/456 [==============================] - 0s 229us/step - loss: 25.9082 - mean_absolute_error: 3.7079\n",
            "Epoch 96/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 25.7226 - mean_absolute_error: 3.5813\n",
            "Epoch 97/100\n",
            "456/456 [==============================] - 0s 215us/step - loss: 25.1323 - mean_absolute_error: 3.6433\n",
            "Epoch 98/100\n",
            "456/456 [==============================] - 0s 217us/step - loss: 25.2275 - mean_absolute_error: 3.5166\n",
            "Epoch 99/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 27.4346 - mean_absolute_error: 3.8615\n",
            "Epoch 100/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 24.3602 - mean_absolute_error: 3.4837\n",
            "50/50 [==============================] - 0s 4ms/step\n",
            "Epoch 1/100\n",
            "456/456 [==============================] - 1s 2ms/step - loss: 18446.6437 - mean_absolute_error: 129.4707\n",
            "Epoch 2/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 3620.2252 - mean_absolute_error: 57.6114\n",
            "Epoch 3/100\n",
            "456/456 [==============================] - 0s 222us/step - loss: 1320.4783 - mean_absolute_error: 32.0750\n",
            "Epoch 4/100\n",
            "456/456 [==============================] - 0s 254us/step - loss: 797.6762 - mean_absolute_error: 24.7916\n",
            "Epoch 5/100\n",
            "456/456 [==============================] - 0s 217us/step - loss: 562.9216 - mean_absolute_error: 20.2124\n",
            "Epoch 6/100\n",
            "456/456 [==============================] - 0s 218us/step - loss: 434.2345 - mean_absolute_error: 17.1728\n",
            "Epoch 7/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 349.6220 - mean_absolute_error: 15.2367\n",
            "Epoch 8/100\n",
            "456/456 [==============================] - 0s 220us/step - loss: 287.4691 - mean_absolute_error: 13.7554\n",
            "Epoch 9/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 243.0739 - mean_absolute_error: 12.6022\n",
            "Epoch 10/100\n",
            "456/456 [==============================] - 0s 248us/step - loss: 206.6153 - mean_absolute_error: 11.5272\n",
            "Epoch 11/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 173.6291 - mean_absolute_error: 10.3318\n",
            "Epoch 12/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 146.5773 - mean_absolute_error: 9.2815\n",
            "Epoch 13/100\n",
            "456/456 [==============================] - 0s 222us/step - loss: 124.2659 - mean_absolute_error: 8.3008\n",
            "Epoch 14/100\n",
            "456/456 [==============================] - 0s 226us/step - loss: 107.0596 - mean_absolute_error: 7.4366\n",
            "Epoch 15/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 94.6393 - mean_absolute_error: 6.8094\n",
            "Epoch 16/100\n",
            "456/456 [==============================] - 0s 229us/step - loss: 84.5627 - mean_absolute_error: 6.3163\n",
            "Epoch 17/100\n",
            "456/456 [==============================] - 0s 217us/step - loss: 78.1901 - mean_absolute_error: 6.1156\n",
            "Epoch 18/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 74.4370 - mean_absolute_error: 6.0291\n",
            "Epoch 19/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 71.6399 - mean_absolute_error: 5.9740\n",
            "Epoch 20/100\n",
            "456/456 [==============================] - 0s 227us/step - loss: 69.8674 - mean_absolute_error: 5.9525\n",
            "Epoch 21/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 68.5580 - mean_absolute_error: 5.9654\n",
            "Epoch 22/100\n",
            "456/456 [==============================] - 0s 213us/step - loss: 67.5138 - mean_absolute_error: 5.9101\n",
            "Epoch 23/100\n",
            "456/456 [==============================] - 0s 238us/step - loss: 66.2630 - mean_absolute_error: 5.8695\n",
            "Epoch 24/100\n",
            "456/456 [==============================] - 0s 199us/step - loss: 65.4292 - mean_absolute_error: 5.8641\n",
            "Epoch 25/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 64.6504 - mean_absolute_error: 5.8521\n",
            "Epoch 26/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 63.9911 - mean_absolute_error: 5.7766\n",
            "Epoch 27/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 62.1160 - mean_absolute_error: 5.6645\n",
            "Epoch 28/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 60.8763 - mean_absolute_error: 5.6263\n",
            "Epoch 29/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 59.8590 - mean_absolute_error: 5.5785\n",
            "Epoch 30/100\n",
            "456/456 [==============================] - 0s 222us/step - loss: 59.4848 - mean_absolute_error: 5.5628\n",
            "Epoch 31/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 58.2708 - mean_absolute_error: 5.4795\n",
            "Epoch 32/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 57.7076 - mean_absolute_error: 5.4492\n",
            "Epoch 33/100\n",
            "456/456 [==============================] - 0s 228us/step - loss: 58.9178 - mean_absolute_error: 5.5526\n",
            "Epoch 34/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 56.2724 - mean_absolute_error: 5.3641\n",
            "Epoch 35/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 55.2754 - mean_absolute_error: 5.3034\n",
            "Epoch 36/100\n",
            "456/456 [==============================] - 0s 231us/step - loss: 54.8882 - mean_absolute_error: 5.2835\n",
            "Epoch 37/100\n",
            "456/456 [==============================] - 0s 196us/step - loss: 54.3694 - mean_absolute_error: 5.3355\n",
            "Epoch 38/100\n",
            "456/456 [==============================] - 0s 197us/step - loss: 54.5057 - mean_absolute_error: 5.2354\n",
            "Epoch 39/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 52.3837 - mean_absolute_error: 5.1338\n",
            "Epoch 40/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 52.3862 - mean_absolute_error: 5.1739\n",
            "Epoch 41/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 51.7759 - mean_absolute_error: 5.1122\n",
            "Epoch 42/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 51.5810 - mean_absolute_error: 5.2245\n",
            "Epoch 43/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 51.2636 - mean_absolute_error: 5.1555\n",
            "Epoch 44/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 50.2815 - mean_absolute_error: 5.0359\n",
            "Epoch 45/100\n",
            "456/456 [==============================] - 0s 199us/step - loss: 50.5310 - mean_absolute_error: 5.1874\n",
            "Epoch 46/100\n",
            "456/456 [==============================] - 0s 223us/step - loss: 49.9342 - mean_absolute_error: 5.0512\n",
            "Epoch 47/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 49.6226 - mean_absolute_error: 5.0585\n",
            "Epoch 48/100\n",
            "456/456 [==============================] - 0s 194us/step - loss: 48.7135 - mean_absolute_error: 5.0190\n",
            "Epoch 49/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 48.0259 - mean_absolute_error: 5.0049\n",
            "Epoch 50/100\n",
            "456/456 [==============================] - 0s 224us/step - loss: 48.3416 - mean_absolute_error: 5.1037\n",
            "Epoch 51/100\n",
            "456/456 [==============================] - 0s 218us/step - loss: 47.8261 - mean_absolute_error: 5.1417\n",
            "Epoch 52/100\n",
            "456/456 [==============================] - 0s 216us/step - loss: 46.4976 - mean_absolute_error: 4.9779\n",
            "Epoch 53/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 45.5448 - mean_absolute_error: 4.7938\n",
            "Epoch 54/100\n",
            "456/456 [==============================] - 0s 256us/step - loss: 45.2635 - mean_absolute_error: 4.8913\n",
            "Epoch 55/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 45.9956 - mean_absolute_error: 4.9181\n",
            "Epoch 56/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 44.9930 - mean_absolute_error: 4.9149\n",
            "Epoch 57/100\n",
            "456/456 [==============================] - 0s 200us/step - loss: 43.9093 - mean_absolute_error: 4.8553\n",
            "Epoch 58/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 43.6185 - mean_absolute_error: 4.7978\n",
            "Epoch 59/100\n",
            "456/456 [==============================] - 0s 193us/step - loss: 42.0214 - mean_absolute_error: 4.7466\n",
            "Epoch 60/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 43.7839 - mean_absolute_error: 4.9852\n",
            "Epoch 61/100\n",
            "456/456 [==============================] - 0s 213us/step - loss: 40.4305 - mean_absolute_error: 4.6502\n",
            "Epoch 62/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 39.7161 - mean_absolute_error: 4.5995\n",
            "Epoch 63/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 39.7834 - mean_absolute_error: 4.6209\n",
            "Epoch 64/100\n",
            "456/456 [==============================] - 0s 231us/step - loss: 38.7231 - mean_absolute_error: 4.5713\n",
            "Epoch 65/100\n",
            "456/456 [==============================] - 0s 199us/step - loss: 37.9224 - mean_absolute_error: 4.5118\n",
            "Epoch 66/100\n",
            "456/456 [==============================] - 0s 215us/step - loss: 38.2846 - mean_absolute_error: 4.5229\n",
            "Epoch 67/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 38.3693 - mean_absolute_error: 4.5851\n",
            "Epoch 68/100\n",
            "456/456 [==============================] - 0s 222us/step - loss: 38.2375 - mean_absolute_error: 4.5801\n",
            "Epoch 69/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 37.9380 - mean_absolute_error: 4.5855\n",
            "Epoch 70/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 36.6166 - mean_absolute_error: 4.3982\n",
            "Epoch 71/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 40.2385 - mean_absolute_error: 4.6885\n",
            "Epoch 72/100\n",
            "456/456 [==============================] - 0s 217us/step - loss: 36.0966 - mean_absolute_error: 4.4113\n",
            "Epoch 73/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 36.9629 - mean_absolute_error: 4.5187\n",
            "Epoch 74/100\n",
            "456/456 [==============================] - 0s 225us/step - loss: 35.4740 - mean_absolute_error: 4.4191\n",
            "Epoch 75/100\n",
            "456/456 [==============================] - 0s 223us/step - loss: 34.9812 - mean_absolute_error: 4.3097\n",
            "Epoch 76/100\n",
            "456/456 [==============================] - 0s 200us/step - loss: 34.4878 - mean_absolute_error: 4.3068\n",
            "Epoch 77/100\n",
            "456/456 [==============================] - 0s 223us/step - loss: 34.9504 - mean_absolute_error: 4.4047\n",
            "Epoch 78/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 33.5415 - mean_absolute_error: 4.2642\n",
            "Epoch 79/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 34.6744 - mean_absolute_error: 4.3639\n",
            "Epoch 80/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 34.2354 - mean_absolute_error: 4.3127\n",
            "Epoch 81/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 34.3807 - mean_absolute_error: 4.3755\n",
            "Epoch 82/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 32.8414 - mean_absolute_error: 4.1597\n",
            "Epoch 83/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 34.1133 - mean_absolute_error: 4.3642\n",
            "Epoch 84/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 32.2525 - mean_absolute_error: 4.1761\n",
            "Epoch 85/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 34.2019 - mean_absolute_error: 4.3607\n",
            "Epoch 86/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 33.2025 - mean_absolute_error: 4.3280\n",
            "Epoch 87/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 32.7001 - mean_absolute_error: 4.2316\n",
            "Epoch 88/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 31.6365 - mean_absolute_error: 4.1657\n",
            "Epoch 89/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 32.2199 - mean_absolute_error: 4.1693\n",
            "Epoch 90/100\n",
            "456/456 [==============================] - 0s 191us/step - loss: 33.2134 - mean_absolute_error: 4.3055\n",
            "Epoch 91/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 31.6915 - mean_absolute_error: 4.1729\n",
            "Epoch 92/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 32.0475 - mean_absolute_error: 4.1809\n",
            "Epoch 93/100\n",
            "456/456 [==============================] - 0s 218us/step - loss: 32.9131 - mean_absolute_error: 4.3418\n",
            "Epoch 94/100\n",
            "456/456 [==============================] - 0s 239us/step - loss: 35.2575 - mean_absolute_error: 4.5251\n",
            "Epoch 95/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 31.2010 - mean_absolute_error: 4.1295\n",
            "Epoch 96/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 31.2654 - mean_absolute_error: 4.2034\n",
            "Epoch 97/100\n",
            "456/456 [==============================] - 0s 216us/step - loss: 33.0495 - mean_absolute_error: 4.3835\n",
            "Epoch 98/100\n",
            "456/456 [==============================] - 0s 221us/step - loss: 31.2777 - mean_absolute_error: 4.1787\n",
            "Epoch 99/100\n",
            "456/456 [==============================] - 0s 199us/step - loss: 32.0843 - mean_absolute_error: 4.1067\n",
            "Epoch 100/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 30.2471 - mean_absolute_error: 4.0541\n",
            "50/50 [==============================] - 0s 4ms/step\n",
            "Epoch 1/100\n",
            "456/456 [==============================] - 1s 2ms/step - loss: 1083.4991 - mean_absolute_error: 23.9907\n",
            "Epoch 2/100\n",
            "456/456 [==============================] - 0s 196us/step - loss: 220.2880 - mean_absolute_error: 11.0911\n",
            "Epoch 3/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 159.3399 - mean_absolute_error: 8.9775\n",
            "Epoch 4/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 135.5712 - mean_absolute_error: 7.9424\n",
            "Epoch 5/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 126.1236 - mean_absolute_error: 7.6676\n",
            "Epoch 6/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 114.4059 - mean_absolute_error: 7.3339\n",
            "Epoch 7/100\n",
            "456/456 [==============================] - 0s 191us/step - loss: 102.2289 - mean_absolute_error: 6.6931\n",
            "Epoch 8/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 95.8160 - mean_absolute_error: 6.7280\n",
            "Epoch 9/100\n",
            "456/456 [==============================] - 0s 195us/step - loss: 87.9312 - mean_absolute_error: 6.4380\n",
            "Epoch 10/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 80.1957 - mean_absolute_error: 6.1109\n",
            "Epoch 11/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 75.6075 - mean_absolute_error: 6.0784\n",
            "Epoch 12/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 70.8551 - mean_absolute_error: 5.8614\n",
            "Epoch 13/100\n",
            "456/456 [==============================] - 0s 217us/step - loss: 67.3459 - mean_absolute_error: 5.7651\n",
            "Epoch 14/100\n",
            "456/456 [==============================] - 0s 200us/step - loss: 64.5496 - mean_absolute_error: 5.7170\n",
            "Epoch 15/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 60.2435 - mean_absolute_error: 5.5776\n",
            "Epoch 16/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 58.1044 - mean_absolute_error: 5.4276\n",
            "Epoch 17/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 55.0967 - mean_absolute_error: 5.4051\n",
            "Epoch 18/100\n",
            "456/456 [==============================] - 0s 220us/step - loss: 56.1192 - mean_absolute_error: 5.4739\n",
            "Epoch 19/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 52.9942 - mean_absolute_error: 5.2771\n",
            "Epoch 20/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 49.4427 - mean_absolute_error: 5.1232\n",
            "Epoch 21/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 48.5479 - mean_absolute_error: 5.0876\n",
            "Epoch 22/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 47.1276 - mean_absolute_error: 5.0611\n",
            "Epoch 23/100\n",
            "456/456 [==============================] - 0s 205us/step - loss: 48.6022 - mean_absolute_error: 5.1728\n",
            "Epoch 24/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 47.5776 - mean_absolute_error: 5.1500\n",
            "Epoch 25/100\n",
            "456/456 [==============================] - 0s 215us/step - loss: 47.2601 - mean_absolute_error: 5.1073\n",
            "Epoch 26/100\n",
            "456/456 [==============================] - 0s 192us/step - loss: 45.5555 - mean_absolute_error: 4.9823\n",
            "Epoch 27/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 46.1295 - mean_absolute_error: 4.9378\n",
            "Epoch 28/100\n",
            "456/456 [==============================] - 0s 199us/step - loss: 45.5910 - mean_absolute_error: 4.9886\n",
            "Epoch 29/100\n",
            "456/456 [==============================] - 0s 198us/step - loss: 46.9428 - mean_absolute_error: 5.1993\n",
            "Epoch 30/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 43.3027 - mean_absolute_error: 4.8882\n",
            "Epoch 31/100\n",
            "456/456 [==============================] - 0s 221us/step - loss: 45.3008 - mean_absolute_error: 4.9798\n",
            "Epoch 32/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 41.3796 - mean_absolute_error: 4.8572\n",
            "Epoch 33/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 43.3141 - mean_absolute_error: 4.9719\n",
            "Epoch 34/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 41.0197 - mean_absolute_error: 4.7637\n",
            "Epoch 35/100\n",
            "456/456 [==============================] - 0s 249us/step - loss: 40.0755 - mean_absolute_error: 4.7925\n",
            "Epoch 36/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 40.2867 - mean_absolute_error: 4.7791\n",
            "Epoch 37/100\n",
            "456/456 [==============================] - 0s 217us/step - loss: 39.4880 - mean_absolute_error: 4.7529\n",
            "Epoch 38/100\n",
            "456/456 [==============================] - 0s 216us/step - loss: 39.0213 - mean_absolute_error: 4.7835\n",
            "Epoch 39/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 43.7689 - mean_absolute_error: 5.1277\n",
            "Epoch 40/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 40.9681 - mean_absolute_error: 4.7530\n",
            "Epoch 41/100\n",
            "456/456 [==============================] - 0s 213us/step - loss: 41.4607 - mean_absolute_error: 4.8549\n",
            "Epoch 42/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 38.1793 - mean_absolute_error: 4.6477\n",
            "Epoch 43/100\n",
            "456/456 [==============================] - 0s 215us/step - loss: 38.5415 - mean_absolute_error: 4.6041\n",
            "Epoch 44/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 41.1728 - mean_absolute_error: 4.8270\n",
            "Epoch 45/100\n",
            "456/456 [==============================] - 0s 227us/step - loss: 39.1351 - mean_absolute_error: 4.7823\n",
            "Epoch 46/100\n",
            "456/456 [==============================] - 0s 223us/step - loss: 39.2204 - mean_absolute_error: 4.6619\n",
            "Epoch 47/100\n",
            "456/456 [==============================] - 0s 235us/step - loss: 37.4266 - mean_absolute_error: 4.5947\n",
            "Epoch 48/100\n",
            "456/456 [==============================] - 0s 238us/step - loss: 43.8142 - mean_absolute_error: 4.9839\n",
            "Epoch 49/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 38.1182 - mean_absolute_error: 4.6461\n",
            "Epoch 50/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 38.2263 - mean_absolute_error: 4.6357\n",
            "Epoch 51/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 37.5040 - mean_absolute_error: 4.6395\n",
            "Epoch 52/100\n",
            "456/456 [==============================] - 0s 216us/step - loss: 35.8865 - mean_absolute_error: 4.5130\n",
            "Epoch 53/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 36.7370 - mean_absolute_error: 4.4892\n",
            "Epoch 54/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 37.0269 - mean_absolute_error: 4.5023\n",
            "Epoch 55/100\n",
            "456/456 [==============================] - 0s 220us/step - loss: 36.4573 - mean_absolute_error: 4.5483\n",
            "Epoch 56/100\n",
            "456/456 [==============================] - 0s 231us/step - loss: 37.6842 - mean_absolute_error: 4.6068\n",
            "Epoch 57/100\n",
            "456/456 [==============================] - 0s 223us/step - loss: 36.3699 - mean_absolute_error: 4.4584\n",
            "Epoch 58/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 39.0414 - mean_absolute_error: 4.7341\n",
            "Epoch 59/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 35.6649 - mean_absolute_error: 4.4763\n",
            "Epoch 60/100\n",
            "456/456 [==============================] - 0s 225us/step - loss: 36.8720 - mean_absolute_error: 4.5870\n",
            "Epoch 61/100\n",
            "456/456 [==============================] - 0s 229us/step - loss: 37.4316 - mean_absolute_error: 4.6006\n",
            "Epoch 62/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 39.7320 - mean_absolute_error: 4.7054\n",
            "Epoch 63/100\n",
            "456/456 [==============================] - 0s 200us/step - loss: 34.9418 - mean_absolute_error: 4.4081\n",
            "Epoch 64/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 35.7867 - mean_absolute_error: 4.4376\n",
            "Epoch 65/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 33.4378 - mean_absolute_error: 4.3478\n",
            "Epoch 66/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 36.5019 - mean_absolute_error: 4.5804\n",
            "Epoch 67/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 34.8273 - mean_absolute_error: 4.3068\n",
            "Epoch 68/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 35.7251 - mean_absolute_error: 4.4464\n",
            "Epoch 69/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 34.9391 - mean_absolute_error: 4.3775\n",
            "Epoch 70/100\n",
            "456/456 [==============================] - 0s 224us/step - loss: 34.3255 - mean_absolute_error: 4.2939\n",
            "Epoch 71/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 34.4757 - mean_absolute_error: 4.3520\n",
            "Epoch 72/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 34.1073 - mean_absolute_error: 4.2938\n",
            "Epoch 73/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 35.4163 - mean_absolute_error: 4.4003\n",
            "Epoch 74/100\n",
            "456/456 [==============================] - 0s 217us/step - loss: 36.0865 - mean_absolute_error: 4.4122\n",
            "Epoch 75/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 34.4089 - mean_absolute_error: 4.3761\n",
            "Epoch 76/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 35.2726 - mean_absolute_error: 4.4612\n",
            "Epoch 77/100\n",
            "456/456 [==============================] - 0s 223us/step - loss: 35.4438 - mean_absolute_error: 4.4328\n",
            "Epoch 78/100\n",
            "456/456 [==============================] - 0s 234us/step - loss: 34.0869 - mean_absolute_error: 4.3549\n",
            "Epoch 79/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 33.3659 - mean_absolute_error: 4.2809\n",
            "Epoch 80/100\n",
            "456/456 [==============================] - 0s 224us/step - loss: 34.7034 - mean_absolute_error: 4.3142\n",
            "Epoch 81/100\n",
            "456/456 [==============================] - 0s 223us/step - loss: 31.8916 - mean_absolute_error: 4.1908\n",
            "Epoch 82/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 34.7935 - mean_absolute_error: 4.3384\n",
            "Epoch 83/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 32.4974 - mean_absolute_error: 4.2563\n",
            "Epoch 84/100\n",
            "456/456 [==============================] - 0s 213us/step - loss: 31.1862 - mean_absolute_error: 4.1069\n",
            "Epoch 85/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 32.1442 - mean_absolute_error: 4.2592\n",
            "Epoch 86/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 35.4352 - mean_absolute_error: 4.4869\n",
            "Epoch 87/100\n",
            "456/456 [==============================] - 0s 217us/step - loss: 31.7045 - mean_absolute_error: 4.1921\n",
            "Epoch 88/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 32.0504 - mean_absolute_error: 4.1087\n",
            "Epoch 89/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 31.9031 - mean_absolute_error: 4.0945\n",
            "Epoch 90/100\n",
            "456/456 [==============================] - 0s 230us/step - loss: 35.7226 - mean_absolute_error: 4.4911\n",
            "Epoch 91/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 32.8456 - mean_absolute_error: 4.2222\n",
            "Epoch 92/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 32.7817 - mean_absolute_error: 4.2523\n",
            "Epoch 93/100\n",
            "456/456 [==============================] - 0s 213us/step - loss: 32.0751 - mean_absolute_error: 4.0783\n",
            "Epoch 94/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 34.2984 - mean_absolute_error: 4.3937\n",
            "Epoch 95/100\n",
            "456/456 [==============================] - 0s 216us/step - loss: 32.6814 - mean_absolute_error: 4.1824\n",
            "Epoch 96/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 31.1263 - mean_absolute_error: 4.0116\n",
            "Epoch 97/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 33.3151 - mean_absolute_error: 4.3809\n",
            "Epoch 98/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 33.3596 - mean_absolute_error: 4.2927\n",
            "Epoch 99/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 32.6166 - mean_absolute_error: 4.2264\n",
            "Epoch 100/100\n",
            "456/456 [==============================] - 0s 225us/step - loss: 31.7471 - mean_absolute_error: 4.1914\n",
            "50/50 [==============================] - 0s 5ms/step\n",
            "Epoch 1/100\n",
            "456/456 [==============================] - 1s 2ms/step - loss: 1028.6062 - mean_absolute_error: 23.9137\n",
            "Epoch 2/100\n",
            "456/456 [==============================] - 0s 215us/step - loss: 219.3249 - mean_absolute_error: 11.7057\n",
            "Epoch 3/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 151.7775 - mean_absolute_error: 9.4580\n",
            "Epoch 4/100\n",
            "456/456 [==============================] - 0s 205us/step - loss: 114.3711 - mean_absolute_error: 8.0746\n",
            "Epoch 5/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 94.6576 - mean_absolute_error: 7.2867\n",
            "Epoch 6/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 86.8456 - mean_absolute_error: 6.9923\n",
            "Epoch 7/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 77.3566 - mean_absolute_error: 6.5366\n",
            "Epoch 8/100\n",
            "456/456 [==============================] - 0s 213us/step - loss: 73.7051 - mean_absolute_error: 6.4336\n",
            "Epoch 9/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 64.6726 - mean_absolute_error: 5.8521\n",
            "Epoch 10/100\n",
            "456/456 [==============================] - 0s 221us/step - loss: 60.5024 - mean_absolute_error: 5.6607\n",
            "Epoch 11/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 57.2815 - mean_absolute_error: 5.4373\n",
            "Epoch 12/100\n",
            "456/456 [==============================] - 0s 194us/step - loss: 54.1934 - mean_absolute_error: 5.4463\n",
            "Epoch 13/100\n",
            "456/456 [==============================] - 0s 237us/step - loss: 54.3814 - mean_absolute_error: 5.3276\n",
            "Epoch 14/100\n",
            "456/456 [==============================] - 0s 221us/step - loss: 50.4739 - mean_absolute_error: 5.1600\n",
            "Epoch 15/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 49.2782 - mean_absolute_error: 5.1633\n",
            "Epoch 16/100\n",
            "456/456 [==============================] - 0s 218us/step - loss: 49.1627 - mean_absolute_error: 5.0830\n",
            "Epoch 17/100\n",
            "456/456 [==============================] - 0s 216us/step - loss: 50.1295 - mean_absolute_error: 5.3977\n",
            "Epoch 18/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 48.7737 - mean_absolute_error: 5.2270\n",
            "Epoch 19/100\n",
            "456/456 [==============================] - 0s 227us/step - loss: 48.8983 - mean_absolute_error: 5.1740\n",
            "Epoch 20/100\n",
            "456/456 [==============================] - 0s 222us/step - loss: 45.6030 - mean_absolute_error: 5.0534\n",
            "Epoch 21/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 46.9873 - mean_absolute_error: 4.9941\n",
            "Epoch 22/100\n",
            "456/456 [==============================] - 0s 224us/step - loss: 57.1879 - mean_absolute_error: 5.6620\n",
            "Epoch 23/100\n",
            "456/456 [==============================] - 0s 220us/step - loss: 46.9840 - mean_absolute_error: 5.1773\n",
            "Epoch 24/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 44.1001 - mean_absolute_error: 4.9590\n",
            "Epoch 25/100\n",
            "456/456 [==============================] - 0s 215us/step - loss: 48.6434 - mean_absolute_error: 5.2817\n",
            "Epoch 26/100\n",
            "456/456 [==============================] - 0s 245us/step - loss: 43.8130 - mean_absolute_error: 4.9616\n",
            "Epoch 27/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 43.6571 - mean_absolute_error: 4.9886\n",
            "Epoch 28/100\n",
            "456/456 [==============================] - 0s 198us/step - loss: 44.6559 - mean_absolute_error: 5.0634\n",
            "Epoch 29/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 41.7433 - mean_absolute_error: 4.8653\n",
            "Epoch 30/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 44.8685 - mean_absolute_error: 5.0622\n",
            "Epoch 31/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 44.7334 - mean_absolute_error: 5.0956\n",
            "Epoch 32/100\n",
            "456/456 [==============================] - 0s 223us/step - loss: 44.4966 - mean_absolute_error: 5.0829\n",
            "Epoch 33/100\n",
            "456/456 [==============================] - 0s 230us/step - loss: 41.8866 - mean_absolute_error: 4.8766\n",
            "Epoch 34/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 41.8473 - mean_absolute_error: 4.8161\n",
            "Epoch 35/100\n",
            "456/456 [==============================] - 0s 220us/step - loss: 42.0458 - mean_absolute_error: 4.9260\n",
            "Epoch 36/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 42.8633 - mean_absolute_error: 5.0071\n",
            "Epoch 37/100\n",
            "456/456 [==============================] - 0s 227us/step - loss: 42.3443 - mean_absolute_error: 4.8977\n",
            "Epoch 38/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 43.2636 - mean_absolute_error: 4.9612\n",
            "Epoch 39/100\n",
            "456/456 [==============================] - 0s 229us/step - loss: 39.8621 - mean_absolute_error: 4.7022\n",
            "Epoch 40/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 41.2572 - mean_absolute_error: 4.7705\n",
            "Epoch 41/100\n",
            "456/456 [==============================] - 0s 225us/step - loss: 39.4585 - mean_absolute_error: 4.7174\n",
            "Epoch 42/100\n",
            "456/456 [==============================] - 0s 224us/step - loss: 44.0289 - mean_absolute_error: 4.9509\n",
            "Epoch 43/100\n",
            "456/456 [==============================] - 0s 220us/step - loss: 42.5575 - mean_absolute_error: 4.8514\n",
            "Epoch 44/100\n",
            "456/456 [==============================] - 0s 223us/step - loss: 42.5653 - mean_absolute_error: 4.9888\n",
            "Epoch 45/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 40.5730 - mean_absolute_error: 4.8133\n",
            "Epoch 46/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 39.5785 - mean_absolute_error: 4.7537\n",
            "Epoch 47/100\n",
            "456/456 [==============================] - 0s 207us/step - loss: 44.7854 - mean_absolute_error: 5.1152\n",
            "Epoch 48/100\n",
            "456/456 [==============================] - 0s 227us/step - loss: 39.4638 - mean_absolute_error: 4.6937\n",
            "Epoch 49/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 38.2865 - mean_absolute_error: 4.6817\n",
            "Epoch 50/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 40.9931 - mean_absolute_error: 4.8383\n",
            "Epoch 51/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 39.4010 - mean_absolute_error: 4.7035\n",
            "Epoch 52/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 39.3151 - mean_absolute_error: 4.7124\n",
            "Epoch 53/100\n",
            "456/456 [==============================] - 0s 211us/step - loss: 53.1691 - mean_absolute_error: 5.5838\n",
            "Epoch 54/100\n",
            "456/456 [==============================] - 0s 222us/step - loss: 37.3673 - mean_absolute_error: 4.5668\n",
            "Epoch 55/100\n",
            "456/456 [==============================] - 0s 204us/step - loss: 38.9542 - mean_absolute_error: 4.5674\n",
            "Epoch 56/100\n",
            "456/456 [==============================] - 0s 217us/step - loss: 41.1283 - mean_absolute_error: 4.8740\n",
            "Epoch 57/100\n",
            "456/456 [==============================] - 0s 220us/step - loss: 38.8253 - mean_absolute_error: 4.6449\n",
            "Epoch 58/100\n",
            "456/456 [==============================] - 0s 215us/step - loss: 38.4111 - mean_absolute_error: 4.6628\n",
            "Epoch 59/100\n",
            "456/456 [==============================] - 0s 225us/step - loss: 36.6092 - mean_absolute_error: 4.5387\n",
            "Epoch 60/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 39.3656 - mean_absolute_error: 4.7209\n",
            "Epoch 61/100\n",
            "456/456 [==============================] - 0s 236us/step - loss: 39.3948 - mean_absolute_error: 4.8076\n",
            "Epoch 62/100\n",
            "456/456 [==============================] - 0s 201us/step - loss: 39.7227 - mean_absolute_error: 4.7106\n",
            "Epoch 63/100\n",
            "456/456 [==============================] - 0s 210us/step - loss: 39.6037 - mean_absolute_error: 4.7752\n",
            "Epoch 64/100\n",
            "456/456 [==============================] - 0s 215us/step - loss: 38.4805 - mean_absolute_error: 4.6703\n",
            "Epoch 65/100\n",
            "456/456 [==============================] - 0s 230us/step - loss: 38.5075 - mean_absolute_error: 4.5571\n",
            "Epoch 66/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 37.6182 - mean_absolute_error: 4.5262\n",
            "Epoch 67/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 37.3345 - mean_absolute_error: 4.5325\n",
            "Epoch 68/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 34.8891 - mean_absolute_error: 4.4464\n",
            "Epoch 69/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 36.6970 - mean_absolute_error: 4.5778\n",
            "Epoch 70/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 35.7215 - mean_absolute_error: 4.4379\n",
            "Epoch 71/100\n",
            "456/456 [==============================] - 0s 202us/step - loss: 37.9885 - mean_absolute_error: 4.6234\n",
            "Epoch 72/100\n",
            "456/456 [==============================] - 0s 220us/step - loss: 34.9289 - mean_absolute_error: 4.3653\n",
            "Epoch 73/100\n",
            "456/456 [==============================] - 0s 208us/step - loss: 36.1170 - mean_absolute_error: 4.4904\n",
            "Epoch 74/100\n",
            "456/456 [==============================] - 0s 218us/step - loss: 38.8975 - mean_absolute_error: 4.6765\n",
            "Epoch 75/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 36.2188 - mean_absolute_error: 4.5517\n",
            "Epoch 76/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 35.0976 - mean_absolute_error: 4.4850\n",
            "Epoch 77/100\n",
            "456/456 [==============================] - 0s 220us/step - loss: 41.4786 - mean_absolute_error: 4.9202\n",
            "Epoch 78/100\n",
            "456/456 [==============================] - 0s 223us/step - loss: 35.8839 - mean_absolute_error: 4.4956\n",
            "Epoch 79/100\n",
            "456/456 [==============================] - 0s 220us/step - loss: 34.5057 - mean_absolute_error: 4.3637\n",
            "Epoch 80/100\n",
            "456/456 [==============================] - 0s 215us/step - loss: 37.2412 - mean_absolute_error: 4.5754\n",
            "Epoch 81/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 39.8929 - mean_absolute_error: 4.8568\n",
            "Epoch 82/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 36.2851 - mean_absolute_error: 4.4886\n",
            "Epoch 83/100\n",
            "456/456 [==============================] - 0s 227us/step - loss: 39.0068 - mean_absolute_error: 4.6127\n",
            "Epoch 84/100\n",
            "456/456 [==============================] - 0s 223us/step - loss: 36.2673 - mean_absolute_error: 4.4650\n",
            "Epoch 85/100\n",
            "456/456 [==============================] - 0s 219us/step - loss: 36.1447 - mean_absolute_error: 4.5149\n",
            "Epoch 86/100\n",
            "456/456 [==============================] - 0s 203us/step - loss: 33.7935 - mean_absolute_error: 4.2287\n",
            "Epoch 87/100\n",
            "456/456 [==============================] - 0s 221us/step - loss: 34.5446 - mean_absolute_error: 4.3022\n",
            "Epoch 88/100\n",
            "456/456 [==============================] - 0s 205us/step - loss: 34.1843 - mean_absolute_error: 4.3678\n",
            "Epoch 89/100\n",
            "456/456 [==============================] - 0s 218us/step - loss: 37.5511 - mean_absolute_error: 4.5623\n",
            "Epoch 90/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 33.3363 - mean_absolute_error: 4.2975\n",
            "Epoch 91/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 32.6978 - mean_absolute_error: 4.2429\n",
            "Epoch 92/100\n",
            "456/456 [==============================] - 0s 229us/step - loss: 33.8244 - mean_absolute_error: 4.3023\n",
            "Epoch 93/100\n",
            "456/456 [==============================] - 0s 213us/step - loss: 36.6918 - mean_absolute_error: 4.5252\n",
            "Epoch 94/100\n",
            "456/456 [==============================] - 0s 216us/step - loss: 36.5172 - mean_absolute_error: 4.4253\n",
            "Epoch 95/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 36.9267 - mean_absolute_error: 4.4922\n",
            "Epoch 96/100\n",
            "456/456 [==============================] - 0s 212us/step - loss: 35.0312 - mean_absolute_error: 4.4039\n",
            "Epoch 97/100\n",
            "456/456 [==============================] - 0s 214us/step - loss: 34.1969 - mean_absolute_error: 4.3965\n",
            "Epoch 98/100\n",
            "456/456 [==============================] - 0s 206us/step - loss: 38.4104 - mean_absolute_error: 4.5788\n",
            "Epoch 99/100\n",
            "456/456 [==============================] - 0s 213us/step - loss: 33.3117 - mean_absolute_error: 4.2501\n",
            "Epoch 100/100\n",
            "456/456 [==============================] - 0s 209us/step - loss: 32.4624 - mean_absolute_error: 4.1878\n",
            "50/50 [==============================] - 0s 6ms/step\n",
            "Results: 46.23 (30.79) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gOgrOT2NLiD",
        "colab_type": "code",
        "outputId": "d1b2a830-2a27-45bb-f2d0-af998f4d53db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Normalized using Standard Scaler\n",
        "np.random.seed(s)\n",
        "estimators = []\n",
        "estimators.append(('standardize' , StandardScaler()))\n",
        "estimators.append(('mlp' , KerasRegressor(build_fn = baseline_model , epochs = 50 , batch_size = 5 , verbose = 0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits = 10 , random_state = s)\n",
        "results = cross_val_score(pipeline , x , y , cv = kfold)\n",
        "print(\"Results: %2f(%2f)MSE\"%(abs(results.mean()),results.std()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results: 26.920138(32.533291)MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZM7IJxHOSSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def larger_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(13 , activation='relu' , input_shape=(13,)))\n",
        "  model.add(Dense(10 , activation='relu'))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(optimizer='Adam' , loss='mse' , metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K2czlOlOmfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6lzKlBlOq4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = KFold(n_splits=10, random_state=s)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0FMOO1FOs8s",
        "colab_type": "code",
        "outputId": "a062a11f-8831-431c-a245-0d390be633e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "results = cross_val_score(pipeline, x, y, cv=kfold)\n",
        "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Larger: -20.79 (23.20) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5nKvAjyOtcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wider_model():\n",
        "  model= Sequential()\n",
        "  model.add(Dense(13 , activation='relu' , input_shape=(13,)))\n",
        "  model.add(Dense(20 , activation='relu'))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(optimizer='Adam' , loss='mse' , metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIuzWIUmPGxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimators = []\n",
        "estimators.append(('standardize' , StandardScaler()))\n",
        "estimators.append(('mlp' , KerasRegressor(build_fn = wider_model , epochs = 50 , batch_size = 5 , verbose = 0)))\n",
        "pipeline = Pipeline(estimators)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfrB_3uPPN1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = KFold(n_splits = 10 , random_state = s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpFDYRpRPQvm",
        "colab_type": "code",
        "outputId": "8e1434e6-7e6f-41cd-a97e-ba08d99d55cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "results = cross_val_score(pipeline , x , y , cv = kfold)\n",
        "print(\"Results: %2f(%2f)MSE\"%(abs(results.mean()),results.std()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results: 22.745742(31.437508)MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmQp4TGTPSzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def overfit_model():\n",
        "  model= Sequential()\n",
        "  model.add(Dense(26 , activation='relu' , input_shape=(13,)))\n",
        "  model.add(Dense(13 , activation='relu'))\n",
        "  model.add(Dense(13 , activation='relu'))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(optimizer= 'Adam' , loss='mse' , metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5tDdj_FQt8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimators = []\n",
        "estimators.append(('standardize' , StandardScaler()))\n",
        "estimators.append(('mlp' , KerasRegressor(build_fn = overfit_model , epochs = 50 , batch_size = 5 , verbose = 0)))\n",
        "pipeline = Pipeline(estimators)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-haxt5ymRN4m",
        "colab_type": "code",
        "outputId": "6b09b888-8f9d-4848-dfd5-2f0bebaa2706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "kfold = KFold(n_splits = 10 , random_state = s)\n",
        "results = cross_val_score(pipeline , x , y , cv = kfold)\n",
        "print(\"Results: %2f(%2f)MSE\"%(abs(results.mean()),results.std()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results: 28.029040(25.588323)MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T1dD5jgRO8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers\n",
        "def tune_model():\n",
        "  model= Sequential()\n",
        "  model.add(Dense(13 , activation='relu' , input_shape=(13,)))\n",
        "  model.add(Dense(6 , activation='relu'))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(optimizer= 'Adam' , loss='mse' , metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcGMUrv3RSDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimators = []\n",
        "estimators.append(('standardize' , StandardScaler()))\n",
        "estimators.append(('mlp' , KerasRegressor(build_fn = tune_model , epochs = 50 , batch_size = 5 , verbose = 0)))\n",
        "pipeline = Pipeline(estimators)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DMRGOIURVYA",
        "colab_type": "code",
        "outputId": "aeabb3d1-a801-43ad-876f-b1be0866e63e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "kfold = KFold(n_splits = 10 , random_state = s)\n",
        "results = cross_val_score(pipeline , x , y , cv = kfold)\n",
        "print(\"Results: %2f(%2f)MSE\"%(abs(results.mean()),results.std()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results: 20.996721(28.198276)MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gfvrahuRVf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input , Dense\n",
        "from keras import regularizers\n",
        "def functional_model():\n",
        "  x= Input(shape=(13,))\n",
        "  z1= Dense(13 ,  kernel_regularizer=regularizers.l2(0.001), activation='relu')(x)\n",
        "  z2= Dense(6 ,  kernel_regularizer=regularizers.l2(0.001), activation='relu')(z1)\n",
        "  y= Dense(1)(z2)\n",
        "  model = Model(x , y)\n",
        "  model.compile(optimizer= 'Adam' , loss='mse' , metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOvjiwQvRc29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimators = []\n",
        "estimators.append(('standardize' , StandardScaler()))\n",
        "estimators.append(('mlp' , KerasRegressor(build_fn = functional_model , epochs = 50 , batch_size = 5 , verbose = 0)))\n",
        "pipeline = Pipeline(estimators)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO94qk7VRfvZ",
        "colab_type": "code",
        "outputId": "b5d38ac1-052d-465b-8da7-362d2b167ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "kfold = KFold(n_splits = 10 , random_state = s)\n",
        "results = cross_val_score(pipeline , x , y , cv = kfold)\n",
        "print(\"Results: %2f(%2f)MSE\"%(abs(results.mean()),results.std()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results: 21.207485(25.159432)MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMXFKQRsRf3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "class Subclass(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Subclass , self).__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(13 , activation=tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(6 , activation=tf.nn.relu)\n",
        "    self.dense3 = tf.keras.layers.Dense(1)\n",
        "  def call(self , inputs):\n",
        "    a=self.dense1(x)\n",
        "    a= self.dense2(a)\n",
        "    return self.dense3(a)\n",
        "def end():\n",
        "  model= Subclass()\n",
        "  model.compile(optimizer='adam' , loss='mse' , metrics=['mae'])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R6URoToRf7c",
        "colab_type": "code",
        "outputId": "b12ce213-5db3-44bc-f685-c462913ce1cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "source": [
        "estimators = []\n",
        "estimators.append(('standardize' , StandardScaler()))\n",
        "estimators.append(('mlp' , KerasRegressor(build_fn = end , epochs = 50 , batch_size = 5 , verbose = 0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits = 10 , random_state = s)\n",
        "results = cross_val_score(pipeline , x , y , cv = kfold)\n",
        "print(\"Results: %2f(%2f)MSE\"%(abs(results.mean()),results.std()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method Subclass.call of <__main__.Subclass object at 0x7fb5e0a33a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Subclass.call of <__main__.Subclass object at 0x7fb5e0a33a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Subclass.call of <__main__.Subclass object at 0x7fb5e0a33a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Subclass.call of <__main__.Subclass object at 0x7fb5e0a33a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:530: FutureWarning: From version 0.22, errors during fit will result in a cross validation score of NaN by default. Use error_score='raise' if you want an exception raised or error_score=np.nan to adopt the behavior from version 0.22.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-178492047405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results: %2f(%2f)MSE\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 232\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [506,1] vs. [5,1]\n\t [[{{node Adam_73/gradients/loss_73/output_1_loss/SquaredDifference_grad/BroadcastGradientArgs}}]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYgRfMDRasfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build Model from scratch\n",
        "complete_data = dataset.copy()\n",
        "data = complete_data[:,0:13]\n",
        "labels = complete_data[:,13]\n",
        "train_data = data[:404]\n",
        "train_labels = labels[:404]\n",
        "test_data = data[404:]\n",
        "test_labels = labels[404:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2PaaHNVazEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHEouEBKa4sK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu',input_shape=(train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3BvqSIja61o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "k=4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCLRC4Eka9B6",
        "colab_type": "code",
        "outputId": "dcd7cd04-4ab9-48fd-eae5-09552bcecd77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  val_targets = train_labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  partial_train_data = np.concatenate(\n",
        "      [train_data[:i * num_val_samples],train_data[(i + 1) * num_val_samples:]],axis=0)\n",
        "  partial_train_targets = np.concatenate( [train_labels[:i * num_val_samples], train_labels[(i + 1) * num_val_samples:]],axis=0)\n",
        "  model = build_model()\n",
        "  model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "  all_scores.append(val_mae)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_-H0AdIa-6_",
        "colab_type": "code",
        "outputId": "65a2b5c1-51d7-425e-f5ae-714eb3390ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "all_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.7776164489217323, 4.952682606064447, 2.511183802444156, 6.291505728617753]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40tvAah8bBqC",
        "colab_type": "code",
        "outputId": "da65a7f8-4b8d-4a6e-bf35-18151231eec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.mean(all_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.383247146512022"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H78x8SCmbBvG",
        "colab_type": "code",
        "outputId": "08f8cdf1-f669-45db-a977-ca34fdeeb58d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model = build_model()\n",
        "model.fit(train_data, train_labels,\n",
        "epochs=80, batch_size=16, verbose=0)\n",
        "test_mse_score, test_mae_score = model.evaluate(test_data, test_labels)\n",
        "print(test_mse_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102/102 [==============================] - 2s 17ms/step\n",
            "68.65716949163699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQm_xlYbcB_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}